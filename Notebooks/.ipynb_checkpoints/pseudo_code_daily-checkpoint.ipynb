{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "845ab395",
   "metadata": {},
   "source": [
    "# Network Analysis on Post Trade data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b4f713",
   "metadata": {},
   "source": [
    "## **variation in topology and stability of the Scale-free Networks over time**\n",
    "\n",
    "The aim of the study is the construction of Social Networks from Settlement Instructions of T2S:\n",
    "\n",
    "- The topological structure of the Network may change over time due to disruptive events.\n",
    "- Two Case-studies are conducted on disruptive events: COVID19 and BTP Italia, BTP Futura emissions.\n",
    "- The identification over time of a Scale-free behavior and a ranking for the most central nodes is conducted.\n",
    "- Moreover, a networks resiliency analysis is performed using random and targeted attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb3c0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import random\n",
    "random.seed(123456789)\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import gc\n",
    "import pickle\n",
    "import collections\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import BDay\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.ticker import MultipleLocator, FixedFormatter, FixedLocator\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import pickle\n",
    "import boto3\n",
    "#from s3 import S3\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "#import utils\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "514e91c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "from scipy.stats import kstest\n",
    "import powerlaw\n",
    "\n",
    "\n",
    "import pyvis\n",
    "from pyvis.network import Network\n",
    "import igraph as ig\n",
    "import leidenalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe687d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"anonnames.txt\")\n",
    "names = []\n",
    "\n",
    "for i in file:\n",
    "    names.append(i.split(\" \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9647c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names = [ \"Telecom\",\n",
    "\"Software\",\n",
    "\"Technology\",\n",
    "\"Hardware\",\n",
    "\"Electronics\",\n",
    "\"Consulting\",\n",
    "\"General\",\n",
    "\"Frontier\",\n",
    "\"Alpha\",\n",
    "\"Industries\",\n",
    "\"Net\",\n",
    "\"People\",\n",
    "\"Star\",\n",
    "\"Bell\",\n",
    "\"Research\",\n",
    "\"Architecture\",\n",
    "\"Building\",\n",
    "\"Construction\",\n",
    "\"Medicine\",\n",
    "\"Hill\",\n",
    "\"Graphics\",\n",
    "\"Analysis\",\n",
    "\"Vision\",\n",
    "\"Contract\",\n",
    "\"Solutions\",\n",
    "\"Advanced\",\n",
    "\"Venture\",\n",
    "\"Innovation\",\n",
    "\"Systems\",\n",
    "\"Solutions\",\n",
    "\"Provider\",\n",
    "\"Design\",\n",
    "\"Internet\",\n",
    "\"Virtual\",\n",
    "\"Vision\",\n",
    "\"Application\",\n",
    "\"Signal\",\n",
    "\"Network\",\n",
    "\"Net\",\n",
    "\"Data\",\n",
    "\"Electronic\",\n",
    "\"Max\",\n",
    "\"Adventure\",\n",
    "\"Atlantic\",\n",
    "\"Pacific\",\n",
    "\"North\",\n",
    "\"East\",\n",
    "\"South\",\n",
    "\"West\",\n",
    "\"Speed\",\n",
    "\"Universal\",\n",
    "\"Galaxy\",\n",
    "\"Future\",\n",
    "\"Digital\",\n",
    "\"Studio\",\n",
    "\"Interactive\",\n",
    "\"Source\",\n",
    "\"Omega\",\n",
    "\"Direct\",\n",
    "\"Resource\",\n",
    "\"Power\",\n",
    "\"Federated\",\n",
    "\"Star\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_types = ('LawFirm', 'Generic', 'Short')\n",
    "last_names = names\n",
    "\n",
    "def create_company_name(biz_type=None):\n",
    "    name = []\n",
    "    if not biz_type:\n",
    "        biz_type = random.choice(company_types)\n",
    "\n",
    "    if biz_type == \"LawFirm\":\n",
    "        name.append( random.choice(last_names)+ \", \" + random.choice(last_names) + \" & \" + \n",
    "                     random.choice(last_names))\n",
    "        name.append('LLP')\n",
    "    else:\n",
    "        for i in range(1,random.randint(2,4)):\n",
    "            rand_name = random.choice(company_names)\n",
    "            if rand_name not in name:\n",
    "                name.append(rand_name)\n",
    "        if biz_type == 'Generic':\n",
    "            name.append(random.choice(company_types))\n",
    "        elif len(name) < 3:\n",
    "            name.append(random.choice(company_names))\n",
    "\n",
    "    return \" \".join(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20314ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_company_name(\"Short\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161fd96",
   "metadata": {},
   "source": [
    "# Data Retrieval\n",
    "\n",
    "The result plots and statistics will be stored in the dirs __/home/ec2-user/SageMaker/Tesi_results/__\n",
    "\n",
    "- monthly subdir for monthly data\n",
    "\n",
    "- daily subdir for daily data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ab6a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_helper = S3(bucket_name='mt-res-prod-ml-bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the directory where the input data is\n",
    "bucket_name = 'mt-res-prod-ml-bucket'\n",
    "\n",
    "#output_path = f's3://{bucket_name}/tmp-andrea-spark'\n",
    "ROOT_PATH = '/home/ec2-user/SageMaker/Tesi_results/'\n",
    "if 'Tesi_results' not in os.listdir('/home/ec2-user/SageMaker/'):\n",
    "    os.mkdir('/home/ec2-user/SageMaker/Tesi_results')\n",
    "\n",
    "PATH_TO_INPUT_DIR=f'TESI/Tesi-Andrea/RAW'\n",
    "PATH_TO_DATASET = f'TESI/Tesi-Andrea/DATA'\n",
    "path_to_root_directory = f's3://{bucket_name}/{PATH_TO_DATASET}'\n",
    "\n",
    "# Define the partition column of the input directory\n",
    "\n",
    "PARTITION_COLS=['dt_business']\n",
    "START_DATE = '2018-05-01'\n",
    "#END_DATE = '2018-05-04'\n",
    "END_DATE = '2021-07-31'\n",
    "\n",
    "anonymize_data = True\n",
    "if anonymize_data:\n",
    "    ANONYMIZE_DICT = dict()\n",
    "    ROOT_PATH = ROOT_PATH+'anonymized/'\n",
    "    os.makedirs(ROOT_PATH, exist_ok=True)\n",
    "    if 'anonymize_companies.pkl' in os.listdir(ROOT_PATH):\n",
    "        with open(ROOT_PATH+'anonymize_companies.pkl', 'rb') as handle:\n",
    "            ANONYMIZE_DICT = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5957421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_dataset(START_DATE, END_DATE):\n",
    "\n",
    "    # Read the input .parquet dataset\n",
    "    raw_df = s3_helper.read_parquet(remote_dir=PATH_TO_INPUT_DIR, partition_cols=PARTITION_COLS, start_date =START_DATE,end_date=END_DATE)   \n",
    "    # Reset the index of the Pandas DataFrame\n",
    "    raw_df = raw_df.reset_index(drop=True)\n",
    "  \n",
    "\n",
    "    # convert column names to lowercase \n",
    "    raw_df.columns = [x.lower() for x in raw_df.columns]    \n",
    "\n",
    "    return raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb9825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(daterange=\"all\",force = False, mode = 'description'):\n",
    "\n",
    "    if mode == 'description':\n",
    "        features_agg = ['dt_business','ds_deli_pty1','ds_rece_pty1','cd_sett','cd_sec_at','ind_etf_mkt','cd_si_xb_type', 'id_isin','ds_isin']\n",
    "        path = '/DAILY'\n",
    "\n",
    "    if mode == 'bic':\n",
    "        features_agg = ['dt_business','id_deli_pty1_bic','id_rece_pty1_bic','cd_sett','cd_sec_at','ind_etf_mkt','cd_si_xb_type']\n",
    "        path = '/BIC'\n",
    "        \n",
    "    if daterange == \"all\":\n",
    "        start_date = datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "        end_date = datetime.strptime(END_DATE, '%Y-%m-%d')\n",
    "\n",
    "    else:\n",
    "        start_date = datetime.strptime(daterange[0], '%Y-%m-%d')\n",
    "        end_date = datetime.strptime(daterange[1], '%Y-%m-%d') \n",
    "\n",
    "\n",
    "    def load_data():\n",
    "        df = pd.DataFrame()\n",
    "        csv_list =[i for i in [i.key.split(\"/\")[-1] for i in s3_helper.bucket.objects.filter(Prefix=PATH_TO_DATASET+'/DAILY')] if datetime.strptime(i.split('.csv')[0], '%Y-%m-%d') >= start_date and  datetime.strptime(i.split('.csv')[0], '%Y-%m-%d') <= end_date ]\n",
    "        df = pd.DataFrame()\n",
    "        print(\"Load data from\", csv_list[0], 'to', csv_list[-1])\n",
    "        \n",
    "        for f in csv_list:\n",
    "            #print(f)\n",
    "            df = pd.concat([pd.read_csv(os.path.join(path_to_root_directory+path,f)),df])\n",
    "        return df\n",
    "\n",
    "    if force: \n",
    "        day = datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "        holidays = []\n",
    "        \n",
    "        while day <= datetime.strptime(END_DATE, '%Y-%m-%d'):\n",
    "            string_day = str(day).split(\" \")[0]  \n",
    "            if day.weekday() < 5:\n",
    "                \n",
    "                try:\n",
    "                    raw_df = read_input_dataset(string_day,string_day)\n",
    "                    raw_group = raw_df.groupby(features_agg)[['am_pend','am_amt']].sum()\n",
    "                    raw_group = raw_group.reset_index()\n",
    "                    #display(raw_group.head())\n",
    "\n",
    "                    print(raw_group.columns)\n",
    "                    utils.upload_dataset_to_aws_s3_v2(df=raw_group, \\\n",
    "                                    bucket='mt-res-prod-ml-bucket',\\\n",
    "                                    prefix=PATH_TO_DATASET+path,\\\n",
    "                                    output_file_name=f'{string_day}.csv',\\\n",
    "                                    index=False, header=True, sep=',', decimal='.')\n",
    "\n",
    "                    print(\"UPLOADED to s3\", f'{string_day}.csv')\n",
    "                    print(string_day)\n",
    "\n",
    "                except ValueError:\n",
    "                    holidays.append(string_day)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            day += timedelta(days=1)\n",
    "        print(\"Festivity:\", holidays)\n",
    "    print(\"Data loading concluded\")\n",
    "\n",
    "    return load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6571f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_anonymized(node_list):\n",
    "\n",
    "    for i in node_list:\n",
    "        if i == 'MONTE TITOLI':\n",
    "             ANONYMIZE_DICT[i] = 'MONTE TITOLI'\n",
    "\n",
    "        if i == 'C.COMP.GARANZIA':\n",
    "             ANONYMIZE_DICT[i] = 'C.COMP.GARANZIA'\n",
    "                \n",
    "        r_name =  create_company_name(\"Short\")\n",
    "\n",
    "        if r_name in ANONYMIZE_DICT.values():\n",
    "            flag = True\n",
    "\n",
    "            while flag :\n",
    "                r_name =  create_company_name(\"Short\")\n",
    "                if r_name not in ANONYMIZE_DICT.values():\n",
    "                    flag = False\n",
    "\n",
    "        if i not in ANONYMIZE_DICT.keys():\n",
    "            ANONYMIZE_DICT[i] = r_name\n",
    "\n",
    "    with open(ROOT_PATH+'anonymize_companies.pkl', 'wb') as handle:\n",
    "            pickle.dump(ANONYMIZE_DICT, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return  [ANONYMIZE_DICT[i] for i in node_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb165f04",
   "metadata": {},
   "source": [
    "# Graph Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82023441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_generate_dict(df,financial_instrument_agg = False, external_agg=False,sett_agg=False):\n",
    "\n",
    "    df_dict = dict()\n",
    "\n",
    "    if not financial_instrument_agg and not sett_agg and not external_agg:\n",
    "        fi_list = df['cd_sec_at'].unique()\n",
    "        sett_list = df['cd_sett'].unique()\n",
    "        xb_list  = df['cd_si_xb_type'].unique()\n",
    "       \n",
    "        for fi in fi_list:\n",
    "            for sett in sett_list:\n",
    "                for xb in xb_list: \n",
    "                    df_dict[fi+\"_\"+sett+\"_\"+xb] = df[ (df['cd_sec_at'] == fi) & (df['cd_sett'] == sett) & (df['cd_si_xb_type'] == xb)]\n",
    "\n",
    "    if financial_instrument_agg and sett_agg and not external_agg:\n",
    "        xb_list  = df['cd_si_xb_type'].unique()\n",
    "  \n",
    "        for xb in xb_list: \n",
    "            df_dict[xb] = df[(df['cd_si_xb_type'] == xb)]\n",
    "        return df_dict \n",
    "\n",
    "    if not financial_instrument_agg and sett_agg and not external_agg:\n",
    "        fi_list = df['cd_sec_at'].unique()\n",
    "        \n",
    "      #  sett_list = df['cd_sett'].unique()\n",
    "        xb_list  = df['cd_si_xb_type'].unique()\n",
    "\n",
    "        for fi in fi_list:\n",
    "         #   for sett in sett_list:\n",
    "                for xb in xb_list: \n",
    "                    df_dict[fi+\"_\"+xb] = df[ (df['cd_sec_at'] == fi) & (df['cd_si_xb_type'] == xb)]\n",
    "        return df_dict\n",
    "\n",
    "    if financial_instrument_agg and sett_agg and external_agg :\n",
    "        sett_list = df['cd_sett'].unique()\n",
    "\n",
    "        for sett in sett_list:\n",
    "            df_dict[sett] = df[ (df['cd_sett'] == sett)]\n",
    "        return df_dict    \n",
    "\n",
    "    \n",
    "    if not financial_instrument_agg and  not sett_agg and external_agg:\n",
    "        fi_list = df['cd_sec_at'].unique()\n",
    "        sett_list = df['cd_sett'].unique()\n",
    "        for fi in fi_list:\n",
    "\n",
    "            for sett in sett_list:\n",
    "                    df_dict[fi+\"_\"+sett] = df[ (df['cd_sec_at'] == fi) & (df['cd_sett'] == sett) ]\n",
    "\n",
    "    \n",
    "    if financial_instrument_agg and not sett_agg and  external_agg:\n",
    "        sett_list = df['cd_sett'].unique()\n",
    "\n",
    "        for sett in sett_list:\n",
    "                df_dict[sett] = df[(df['cd_sett'] == sett) ]\n",
    "\n",
    "\n",
    "    if financial_instrument_agg and not sett_agg and not external_agg:\n",
    "        sett_list = df['cd_sett'].unique()\n",
    "        xb_list  = df['cd_si_xb_type'].unique()\n",
    "\n",
    "        for sett in sett_list:\n",
    "            for xb in xb_list: \n",
    "                    df_dict[sett+\"_\"+xb] = df[ (df['cd_sec_at'] == fi) & (df['cd_sett'] == sett) & (df['cd_si_xb_type'] == xb)]\n",
    "\n",
    "    \n",
    "    if financial_instrument_agg and not sett_agg and not external_agg :\n",
    "        sett_list = df['cd_sett'].unique()\n",
    "        xb_list = df['cd_si_xb_type'].unique()\n",
    "\n",
    "        for sett in sett_list:\n",
    "            for xb in xb_list: \n",
    "                    df_dict[sett+\"_\"+xb] = df[  (df['cd_sett'] == sett) & (df['cd_si_xb_type'] == xb)]\n",
    "\n",
    "                    \n",
    "    if financial_instrument_agg and not sett_agg and external_agg :\n",
    "        sett_list = df['cd_sett'].unique()\n",
    "        \n",
    "        for sett in sett_list:\n",
    "            df_dict[sett] = df[ (df['cd_sett'] == sett)]\n",
    "        return df_dict\n",
    "\n",
    "    if financial_instrument_agg and  sett_agg and external_agg:\n",
    "        df_dict['all'] = df     \n",
    "\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad1dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_to_graph(df, sett_agg=False):\n",
    "\n",
    "    df['am_pend']= df['am_pend'].astype(int)\n",
    "\n",
    "    df['am_amt']=  df['am_amt'].astype(int)\n",
    "\n",
    "    \n",
    "\n",
    "    df = df[(df['am_amt'] != 0) | (df['am_pend'] != 0 )]\n",
    "\n",
    "    \n",
    "\n",
    "    if  sett_agg== False:\n",
    "\n",
    "        df = df[(df['cd_sett'] == \"S\") | (df['cd_sett']==\"N\")]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272bdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, date_range='all', external_agg = False, financial_instrument_agg = False,sett_agg=False, direction= 'D', mode = None):\n",
    "\n",
    "    if mode == 'bic':\n",
    "\n",
    "        all_musk = ['dt_business','id_deli_pty1_bic','id_rece_pty1_bic','cd_sett','cd_sec_at','cd_si_xb_type']\n",
    "\n",
    "        src_name = 'id_deli_pty1_bic'\n",
    "\n",
    "        dst_name = 'id_rece_pty1_bic'\n",
    "\n",
    "    else:\n",
    "\n",
    "        all_musk = ['dt_business','ds_deli_pty1','ds_rece_pty1','cd_sett','cd_sec_at','cd_si_xb_type']\n",
    "\n",
    "        src_name = 'ds_deli_pty1'\n",
    "\n",
    "        dst_name = 'ds_rece_pty1'        \n",
    "\n",
    "    if date_range == 'all':\n",
    "\n",
    "        all_musk.remove('dt_business')\n",
    "\n",
    "    if date_range == 'Y':\n",
    "\n",
    "        return\n",
    "\n",
    "    if date_range == 'M':\n",
    "\n",
    "        return\n",
    "\n",
    "    if date_range == 'W':\n",
    "\n",
    "        return \n",
    "\n",
    "    if date_range == 'D':\n",
    "\n",
    "        return\n",
    "\n",
    "    \n",
    "\n",
    "    if external_agg:\n",
    "\n",
    "        all_musk.remove('cd_si_xb_type')\n",
    "\n",
    "        \n",
    "\n",
    "    if financial_instrument_agg:\n",
    "\n",
    "        all_musk.remove('cd_sec_at')\n",
    "\n",
    "        \n",
    "\n",
    "    if sett_agg:\n",
    "\n",
    "        all_musk.remove('cd_sett')\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "    df = df.groupby(all_musk)[['am_pend','am_amt']].sum().reset_index()\n",
    "\n",
    "    \n",
    "\n",
    "    df_pp = preprocess_to_graph(df, sett_agg=sett_agg )\n",
    "\n",
    "    df_pp_dict = df_generate_dict(df_pp,external_agg= external_agg,financial_instrument_agg=financial_instrument_agg, sett_agg=sett_agg)\n",
    "\n",
    "    \n",
    "\n",
    "    anonmized_dict = dict()\n",
    "\n",
    "\n",
    "\n",
    "    graph_dict = dict()\n",
    "\n",
    "    for df_name in df_pp_dict:\n",
    "\n",
    "        src_list = list(df_pp_dict[df_name][src_name])\n",
    "\n",
    "        dst_list = list(df_pp_dict[df_name][dst_name])\n",
    "\n",
    "        \n",
    "\n",
    "        if anonymize_data:\n",
    "\n",
    "            src_list = check_anonymized(src_list)\n",
    "\n",
    "            dst_list = check_anonymized(dst_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        w_list = list(df_pp_dict[df_name]['am_amt'])\n",
    "\n",
    "       \n",
    "\n",
    "        if direction == 'D':\n",
    "\n",
    "            G = nx.DiGraph()\n",
    "\n",
    "        else:\n",
    "\n",
    "            G = nx.Graph()\n",
    "\n",
    "        assert(len(src_list) == len(dst_list) == len(w_list))\n",
    "\n",
    "        for index in range(len(src_list)):\n",
    "\n",
    "            src =  src_list[index]\n",
    "\n",
    "            dst = dst_list[index]\n",
    "\n",
    "\n",
    "\n",
    "            w = w_list[index]\n",
    "\n",
    "            \n",
    "\n",
    "         \n",
    "\n",
    "            if G.has_edge(src,dst):\n",
    "\n",
    "                    raise Exception(src,\"_\",dst,\" edge already in the graph\")\n",
    "\n",
    "                    \n",
    "\n",
    "            if src is None or dst is None or w ==0:\n",
    "\n",
    "                print(\"NONE or ZERO\")\n",
    "\n",
    "                raise Exception(\"Zero or None value in the edge\")\n",
    "\n",
    "      \n",
    "\n",
    "            else:\n",
    "\n",
    "\n",
    "\n",
    "                G.add_weighted_edges_from([(src,dst,int(w))])\n",
    "\n",
    "       \n",
    "\n",
    "        assert len(G.nodes()) ==len(list(set(df_pp_dict[df_name][src_name].unique()) | set(df_pp_dict[df_name][dst_name].unique()))), \"Number of nodes and company names must be equal\"\n",
    "\n",
    "        assert len(G.edges())== len(df_pp_dict[df_name]) , \"Number of edges and number of dataframe rows must be equal\"\n",
    "\n",
    "\n",
    "\n",
    "        graph_dict[df_name] = G\n",
    "\n",
    "    return graph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8e3b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_power_law_exponent(degree_list):        \n",
    "\n",
    "    fit = powerlaw.Fit(degree_list)\n",
    "\n",
    "    \n",
    "\n",
    "    alpha = fit.power_law.alpha\n",
    "\n",
    "    xmin = fit.power_law.xmin\n",
    "\n",
    "    test, p = kstest(degree_list, \"powerlaw\", args = (alpha,xmin),N=len(degree_list))\n",
    "\n",
    "    return {'p-value':p,'test':test,'exp': alpha}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tenth_centrality(G, centr_type ,w = None):\n",
    "\n",
    "    if centr_type == 'degree':\n",
    "\n",
    "        centr = nx.degree_centrality(G)\n",
    "\n",
    "    elif centr_type == 'closeness':\n",
    "\n",
    "        centr = nx.closeness_centrality(G)\n",
    "\n",
    "    elif centr_type == 'betweenness':\n",
    "\n",
    "        centr = nx.betweenness_centrality(G, weight= w)\n",
    "\n",
    "    elif centr_type == 'eigenvector':\n",
    "\n",
    "        try:\n",
    "\n",
    "            centr = nx.eigenvector_centrality(G, weight= w)\n",
    "\n",
    "        except:\n",
    "\n",
    "            centr = \"No Eigenvector Centrality\"\n",
    "\n",
    "            return centr\n",
    "\n",
    "    elif centr_type == 'pagerank':\n",
    "\n",
    "        centr = nx.pagerank(G, weight= w)\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise Exception(\"Centrality type not found [try degree, betweenness, closeness, eigenvector, pagerank]\")\n",
    "\n",
    "    sort_orders = sorted(centr.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sort_orders[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_graph_stats(G, name=\"\",str_to_write = \"\"):\n",
    "\n",
    "    \n",
    "\n",
    "    df = dict()\n",
    "\n",
    "    str_to_write = \"\"\n",
    "\n",
    "    order = G.order()\n",
    "\n",
    "    size = G.size()\n",
    "\n",
    "    if order <= 0: \n",
    "\n",
    "        return {\"string_to_write\":\"Not applicable, size or order are equal to 0\"}\n",
    "\n",
    "    \n",
    "\n",
    "    if order <=3:\n",
    "\n",
    "        return {\"string_to_write\":\"Not applicable, size or order lesser than 3\"}\n",
    "\n",
    "    try:\n",
    "\n",
    "        order_size_ratio = size/order\n",
    "\n",
    "      \n",
    "\n",
    "    except:\n",
    "\n",
    "          order_size_ratio = \"NA\"\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    str_to_write+=f'\\n▸Number of nodes: {order} - Number of links:{size} - Size/Order ratio: {order_size_ratio}'\n",
    "\n",
    "    degree= list(dict(G.degree()).values())\n",
    "\n",
    "    \n",
    "\n",
    "    str_to_write+=f'\\n▸Standard deviation: {np.std(degree)}'\n",
    "\n",
    "    str_to_write+= f'\\n▸Mean: {np.mean(degree)}'\n",
    "\n",
    "    str_to_write+= f'\\n▸Median: {np.median(degree)}'\n",
    "\n",
    "    str_to_write+= f'\\n▸Min: {np.min(degree)}'\n",
    "\n",
    "    str_to_write+= f'\\n▸Max: {np.max(degree)}'\n",
    "\n",
    "    \n",
    "\n",
    "    in_degree = list(dict(G.in_degree()).values())\n",
    "\n",
    "    str_to_write+=f'\\n▸Standard deviation in_degree: {np.std(in_degree)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Mean in_degree: {np.mean(in_degree)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Median in_degree: {np.median(in_degree)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Min in_degree: {np.min(in_degree)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Max in_degree: {np.max(in_degree)}'\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    out_degree = list(dict(G.out_degree()).values())\n",
    "\n",
    "    str_to_write+=f'\\n▸Standard deviation out_degree: {np.std(out_degree)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Mean out_degree: {np.mean(out_degree)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Median out_degree: {np.median(out_degree)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Min out_degree: {np.min(out_degree)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Max out_degree: {np.max(out_degree)}'\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    degree_weighted= list(dict(G.degree(weight=\"weight\")).values())\n",
    "\n",
    "    str_to_write+=f'\\n▸Standard deviation weighted: {np.std(degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Mean weighted: {np.mean(degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Median weighted: {np.median(degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Min weighted: {np.min(degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Max weighted: {np.max(degree_weighted)}'\n",
    "\n",
    "    \n",
    "\n",
    "    in_degree_weighted = list(dict(G.in_degree(weight=\"weight\")).values())\n",
    "\n",
    "    str_to_write+=f'\\n▸Standard deviation in_degree weighted: {np.std(in_degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Mean in_degree weighted: {np.mean(in_degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Median in_degree weighted: {np.median(in_degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Min in_degree weighted: {np.min(in_degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Max in_degree weighted: {np.max(in_degree_weighted)}'\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    out_degree_weighted = list(dict(G.out_degree(weight=\"weight\")).values())\n",
    "\n",
    "    str_to_write+=f'\\n▸Standard deviation out_degree weighted: {np.std(out_degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Mean out_degree weighted: {np.mean(out_degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Median out_degree weighted: {np.median(out_degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Min out_degree weighted: {np.min(out_degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸Max out_degree weighted: {np.max(out_degree_weighted)}'\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    density = nx.density(G)\n",
    "\n",
    "    str_to_write+=f'\\n▸Density: {density}'\n",
    "\n",
    "\n",
    "\n",
    "    avg_clustering = nx.average_clustering(G)\n",
    "\n",
    "    str_to_write+=f'\\n▸Avg. Clustering coeff: {avg_clustering}'\n",
    "\n",
    "    transitivity = nx.transitivity(G)\n",
    "\n",
    "    str_to_write+=f'\\n▸Transitivity: {transitivity}'\n",
    "\n",
    "    assortativity =  str(nx.degree_assortativity_coefficient(G))\n",
    "\n",
    "    str_to_write+=f'\\n▸Assortativity coefficient: {assortativity}'\n",
    "\n",
    "    \n",
    "\n",
    "    assortativity_w =  str(nx.degree_assortativity_coefficient(G,weight='weigth'))\n",
    "\n",
    "    str_to_write+=f'\\n▸Assortativity weighted coefficient: {assortativity_w}'\n",
    "\n",
    "    \n",
    "\n",
    "    pearson_assortativity = nx.degree_pearson_correlation_coefficient(G)\n",
    "\n",
    "    str_to_write+=f'\\n▸Pearson Assortativity coefficient: {pearson_assortativity}' \n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        avg_shortest_path_length = nx.average_shortest_path_length(G)\n",
    "\n",
    "    except nx.NetworkXError:\n",
    "\n",
    "        avg_shortest_path_length = \"is weakly connected\"\n",
    "\n",
    "        \n",
    "\n",
    "    dag = nx.is_directed_acyclic_graph(G)\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        diameter = nx.algorithms.distance_measures.diameter(G)\n",
    "\n",
    "    except:\n",
    "\n",
    "        diameter = float(\"inf\")\n",
    "\n",
    "\n",
    "\n",
    "    wk_comps = [len(c) for c in sorted(nx.weakly_connected_components(G),key=len, reverse=True)]\n",
    "\n",
    "    is_weak =  nx.is_strongly_connected(G)\n",
    "\n",
    "    sg_comps = [len(c) for c in sorted(nx.strongly_connected_components(G),key=len, reverse=True)]\n",
    "\n",
    "    is_strong = nx.is_weakly_connected(G)\n",
    "\n",
    "    \n",
    "\n",
    "    str_to_write+=f'\\n▸Average Shortest Path Length: {avg_shortest_path_length}'\n",
    "\n",
    "    str_to_write+= f'\\n▸Diameter: {diameter}'\n",
    "\n",
    "    str_to_write+= f'\\n▸Is DAG?: {dag}'\n",
    "\n",
    "\n",
    "\n",
    "    str_to_write+= f'\\n▸Weakly Connected Components: {wk_comps}'\n",
    "\n",
    "    str_to_write+= f'\\n▸Is Weakly connected?:  {is_weak}'\n",
    "\n",
    "    \n",
    "\n",
    "    str_to_write+= f'\\n▸Strongly Connected Components: {sg_comps}'\n",
    "\n",
    "    str_to_write+= f'\\n▸Is Strongly connected?:  {is_strong}'   \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    deg_centr = nx.degree_centrality(G)\n",
    "\n",
    "    sort_orders_dc = sorted(deg_centr.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "    #for i in range(10):\n",
    "\n",
    "        #print(sort_orders_dc[i])\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    degree_Centrality = tenth_centrality(G, centr_type=\"degree\")\n",
    "\n",
    "    betweenesCentrality = tenth_centrality(G, centr_type=\"betweenness\")\n",
    "\n",
    "    closenessCentrality = tenth_centrality(G, centr_type=\"closeness\")\n",
    "\n",
    "    eigenCentrality = tenth_centrality(G, centr_type=\"eigenvector\")\n",
    "\n",
    "    pagerankCentrality = tenth_centrality(G, centr_type=\"pagerank\")\n",
    "\n",
    "    \n",
    "\n",
    "    betweenesCentrality_w = tenth_centrality(G, centr_type=\"betweenness\", w='weight')\n",
    "\n",
    "    eigenCentrality_w = tenth_centrality(G, centr_type=\"eigenvector\", w='weight')\n",
    "\n",
    "    pagerankCentrality_w = tenth_centrality(G, centr_type=\"pagerank\", w='weight')\n",
    "\n",
    "\n",
    "\n",
    "    str_to_write+=f'\\n▸10 most important nodes for Degree Centrality:\\n{degree_Centrality}'\n",
    "\n",
    "    str_to_write+=f'\\n▸10 most important nodes for Betweennes Centrality:\\n{betweenesCentrality}'\n",
    "\n",
    "    str_to_write+=f'\\n▸10 most important nodes for Closeness Centrality:\\n{closenessCentrality}'\n",
    "\n",
    "    str_to_write+=f'\\n▸10 most important nodes for Eigenvector Centrality:\\n{eigenCentrality}'\n",
    "\n",
    "    str_to_write+=f'\\n▸10 most important nodes for Page Rank:\\n{pagerankCentrality}'\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    str_to_write+=f'\\n▸10 most important nodes for Betweennes Centrality Weighted:\\n{betweenesCentrality}'\n",
    "\n",
    "    str_to_write+=f'\\n▸10 most important nodes for Eigenvector Centrality Weighted:\\n{eigenCentrality}'\n",
    "\n",
    "    str_to_write+=f'\\n▸10 most important nodes for Page Rank Weighted:\\n{pagerankCentrality}'\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    percentile_90 = np.percentile(degree,90)\n",
    "\n",
    "    str_to_write+=f'\\n▸90-percentile degree: {percentile_90}'\n",
    "\n",
    "    hub_nodi = [k for k,v in dict(G.degree()).items() if v>= percentile_90]\n",
    "\n",
    "    str_to_write+=f'\\n▸Number of nodes in HUBs: {len(hub_nodi)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸List of nodes in HUBs:\\n{list(hub_nodi)}'\n",
    "\n",
    "\n",
    "\n",
    "    percentile_90_in = np.percentile(in_degree,90)\n",
    "\n",
    "    str_to_write+=f'\\n▸90-percentile degree: {percentile_90_in}'\n",
    "\n",
    "    hub_nodi_in = [k for k,v in dict(G.degree()).items() if v>= percentile_90_in]\n",
    "\n",
    "    str_to_write+=f'\\n▸Number of nodes in HUBs: {len(hub_nodi_in)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸List of nodes in HUBs:\\n{list(hub_nodi_in)}'\n",
    "\n",
    "\n",
    "\n",
    "    percentile_90_out = np.percentile(out_degree,90)\n",
    "\n",
    "    str_to_write+=f'\\n▸90-percentile degree: {percentile_90_out}'\n",
    "\n",
    "    hub_nodi_out = [k for k,v in dict(G.degree()).items() if v>= percentile_90_out]\n",
    "\n",
    "    str_to_write+=f'\\n▸Number of nodes in HUBs: {len(hub_nodi_out)}'\n",
    "\n",
    "    str_to_write+=f'\\n▸List of nodes in HUBs:\\n{list(hub_nodi_out)}'\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    isolates = list(nx.isolates(G))\n",
    "\n",
    "    str_to_write+=f'\\n▸Isolated nodes:{isolates}'\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # Not working on directed Graphs\n",
    "\n",
    "    #print(\"Network connected?\",nx.is_connected(G))\n",
    "\n",
    "    #print(\"# Connected components\",nx.number_connected_components(G))\n",
    "\n",
    "    #triangles = len(nx.triangles(G))\n",
    "\n",
    "    #print(\"Number of triangles:\",triangles)\n",
    "\n",
    "   \n",
    "\n",
    " #   deg_PL = compute_power_law_exponent(degree)\n",
    "\n",
    " #   deg_W_PL = compute_power_law_exponent(degree_weighted)\n",
    "\n",
    " #   deg_in_PL = compute_power_law_exponent(in_degree)\n",
    "\n",
    " #   deg_in_W_PL= compute_power_law_exponent(in_degree_weighted)\n",
    "\n",
    " #   deg_out_PL= compute_power_law_exponent(out_degree)\n",
    "\n",
    " #   deg_out_W_PL = compute_power_law_exponent(out_degree_weighted)\n",
    "\n",
    "    \n",
    "\n",
    "  #  str_to_write+=f'\\n▸K-test for PowerLaw distribution'\n",
    "\n",
    "\n",
    "\n",
    " #   str_to_write+=f'\\n▸Power Law K-test on Degree: {deg_PL}'\n",
    "\n",
    " #   str_to_write+=f'\\n▸Power Law K-test on In-Degree: {deg_in_PL}'\n",
    "\n",
    " #   str_to_write+=f'\\n▸Power Law K-test on Weighted In-Degree: {deg_in_W_PL}'\n",
    "\n",
    " #   str_to_write+=f'\\n▸Power Law K-test on Out-Degree: {deg_out_PL}'\n",
    "\n",
    " #  str_to_write+=f'\\n▸Power Law K-test on Weighted Out-Degree: {deg_out_W_PL}'\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return {'string_to_write':str_to_write,'order':order,'size':size, 'order_size_ratio': order_size_ratio, 'avg_shortest_path_length': avg_shortest_path_length,\\\n",
    "\n",
    "            'mean':np.mean(degree),'std':np.std(degree),'median':np.median(degree),'min_deg':np.min(degree),'max_deg':np.max(degree),\\\n",
    "\n",
    "            \\\n",
    "\n",
    "            'mean_in':np.mean(in_degree),'std_in':np.std(in_degree),\\\n",
    "\n",
    "            'median_in':np.median(in_degree),'min_deg_in':np.min(in_degree),'max_deg_in':np.max(in_degree),\\\n",
    "\n",
    "            \\\n",
    "\n",
    "            'mean_out':np.mean(out_degree),'std_out':np.std(out_degree),\\\n",
    "\n",
    "            'median_out':np.median(out_degree),'min_deg_out':np.min(out_degree),'max_deg_out':np.max(out_degree),\\\n",
    "\n",
    "            \\\n",
    "\n",
    "            'mean_weighted':np.mean(degree_weighted),'std_weighted':np.std(degree_weighted),\\\n",
    "\n",
    "            'median_weighted':np.median(degree_weighted),'min_deg_weighted':np.min(degree_weighted),'max_deg_weighted':np.max(degree_weighted),\\\n",
    "\n",
    "            \\\n",
    "\n",
    "            'mean_in_weighted':np.mean(in_degree_weighted),'std_in_weighted':np.std(in_degree_weighted),\\\n",
    "\n",
    "            'median_in_weighted':np.median(in_degree_weighted),'min_deg_in_weighted':np.min(in_degree_weighted),'max_deg_in_weighted':np.max(in_degree_weighted),\\\n",
    "\n",
    "            \\\n",
    "\n",
    "            'mean_out_weighted':np.mean(out_degree_weighted),'std_out_weighted':np.std(out_degree_weighted),\\\n",
    "\n",
    "            'median_out_weighted':np.median(out_degree_weighted),'min_deg_out_weighted':np.min(out_degree_weighted),'max_deg_out_weighted':np.max(out_degree_weighted),\\\n",
    "\n",
    "            \\\n",
    "\n",
    "            'assortativity':assortativity, 'pearson_assortativity':pearson_assortativity, 'assortativity_weigthed':assortativity_w, \\\n",
    "\n",
    "            'transitivity':transitivity,'avg_clustering':avg_clustering,'density':density,\\\n",
    "\n",
    "            'diameter':diameter, 'is_dag':dag, 'wk_comps':wk_comps, 'is_weak':is_weak, 'sg_comps':sg_comps, 'is_strong':is_strong,\\\n",
    "\n",
    "            'degree_centrality':degree_Centrality,'betweennes_centrality':betweenesCentrality, 'closeness_centrality':closenessCentrality, \\\n",
    "\n",
    "            'eigen_centrality':eigenCentrality,'pagerank_centrality':pagerankCentrality, \\\n",
    "\n",
    "            'betweennes_centrality_weighted':betweenesCentrality_w,'eigen_centrality_weighted':eigenCentrality_w,\\\n",
    "\n",
    "            'pagerank_centrality_weighted':pagerankCentrality_w,\\\n",
    "\n",
    "            '90-percentile_degree':percentile_90,'hubs':list(hub_nodi),\\\n",
    "\n",
    "            'hubs_number':len(hub_nodi),'isolated_nodes':isolates, \\\n",
    "\n",
    "#'PL_degree_p':deg_PL['p-value'], 'PL_degree_t':deg_PL['test'],'PL_degree_exp':deg_PL['exp'], \\\n",
    "\n",
    "          #  'PL_degree_weighted_p': deg_W_PL['p-value'],'PL_degree_weighted_t': deg_W_PL['test'], 'PL_degree_weighted_exp': deg_W_PL['exp'],  \\\n",
    "\n",
    "          #  'PL_in_degree_p': deg_in_PL['p-value'], 'PL_in_degree_t': deg_in_PL['test'], 'PL_in_degree_exp': deg_in_PL['exp'],\\\n",
    "\n",
    "          #  'PL_in_degree_weighted_p': deg_in_W_PL['p-value'],'PL_in_degree_weighted_t': deg_in_W_PL['test'],'PL_in_degree_weighted_exp': deg_in_W_PL['exp'],\\\n",
    "\n",
    "          #  'PL_out_degree_p': deg_out_PL['p-value'],  'PL_out_degree_t': deg_out_PL['test'], 'PL_out_degree_exp': deg_out_PL['exp'],\\\n",
    "\n",
    "         #   'PL_out_degree_weighted_p': deg_out_W_PL['p-value'], 'PL_out_degree_weighted_t': deg_out_W_PL['test'],'PL_out_degree_weighted_exp': deg_out_W_PL['exp'] \\\n",
    "\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0bc483",
   "metadata": {},
   "source": [
    "# **Internal and External Instruction Analysis**\n",
    "\n",
    "Daily aggregation: for each business day the data are aggregated for Deliver and Receiver name, Cross Border Instruction Indicator, Type of financial instrument, Status of the instruction. For each of this feature different networks are obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_graph(daily_df, mode = None,ea = False, fi = False, sa=True ):\n",
    "\n",
    "    G_dict = dict()\n",
    "\n",
    "    for dt in list(daily_df['dt_business'].unique()):\n",
    "\n",
    "        G_dict[dt] = dict()\n",
    "\n",
    "     \n",
    "\n",
    "        df = daily_df[daily_df['dt_business'] == dt]\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "        G_dict[dt] = create_graph(df,external_agg=ea,financial_instrument_agg=fi,sett_agg = sa, mode = mode)\n",
    "\n",
    "        if fi == False:\n",
    "\n",
    "            G_dict_ETF = create_graph(df[df['ind_etf_mkt'] == 1],external_agg = ea, financial_instrument_agg = True,sett_agg=sa, mode= mode)\n",
    "\n",
    "        \n",
    "\n",
    "     \n",
    "\n",
    "        \n",
    "\n",
    "            for key in list(G_dict_ETF.keys()):\n",
    "\n",
    "                G_dict_ETF[\"ETF_\"+key] = G_dict_ETF.pop(key)\n",
    "\n",
    "        else:\n",
    "\n",
    "            G_dict_ETF = dict()\n",
    "\n",
    "\n",
    "\n",
    "        G_dict[dt] = {**G_dict_ETF, **G_dict[dt]}\n",
    "\n",
    "    return G_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats_distr(df,feature, comparison =False, comparison_splitdate = None, name=\"\",  save = False,save_name =\"\", trend_line=True):\n",
    "\n",
    "    x = [i.split(\"_\")[0] for i in list(df.index)]\n",
    "\n",
    "    y = list(df[feature])\n",
    "\n",
    "    x_init = x\n",
    "\n",
    "    xd = mdates.date2num(list(pd.to_datetime(x)))\n",
    "\n",
    "    y_init = y\n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    " \n",
    "\n",
    "    labels = [feature]\n",
    "\n",
    "#     cmap = cm.get_cmap('tab20c', 1)\n",
    "\n",
    "    colr = ['blue','red','green'] #[cmap.colors[0]]\n",
    "\n",
    "    label2 = None\n",
    "\n",
    "    if comparison:\n",
    "\n",
    "        if comparison_splitdate is None:\n",
    "\n",
    "            raise Expception(\"If you want to compare before and after, please specify the comparison_splitdate parameter\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            while True:\n",
    "\n",
    "                try:\n",
    "\n",
    "               #     print(comparison_splitdate)\n",
    "\n",
    "               #     print(type(comparison_splitdate))\n",
    "\n",
    "                    if type(comparison_splitdate) is tuple:\n",
    "\n",
    "                      \n",
    "\n",
    "                        index = [x.index(i) for i in comparison_splitdate]\n",
    "\n",
    "                        part1 = index[0]\n",
    "\n",
    "                        part2 = index[1]\n",
    "\n",
    "        \n",
    "\n",
    "                        x2 = x[part1:part2+1]\n",
    "\n",
    "                        x3 = x[part2:] \n",
    "\n",
    "                        x = x[:part1+1]\n",
    "\n",
    "\n",
    "\n",
    "                        y2 = y[part1:part2+1]\n",
    "\n",
    "                        y3 = y[part2:]\n",
    "\n",
    "                        y = y[:part1+1]\n",
    "\n",
    "                    \n",
    "\n",
    "                        \n",
    "\n",
    "                        #cmap = cm.get_cmap('tab20c', len(index)+1)\n",
    "\n",
    "                        #colr = cmap.colors\n",
    "\n",
    "                        labels = [feature+\" Before\",feature+\" BTP Emission\",feature+ \" After\"]\n",
    "\n",
    "                    if type(comparison_splitdate) is str:\n",
    "\n",
    "                        index= x.index(comparison_splitdate)\n",
    "\n",
    "                        x2 = x[index:]\n",
    "\n",
    "                        x = x[:index+1]\n",
    "\n",
    "                        y2 = y[index:]\n",
    "\n",
    "                        y = y[:index+1]\n",
    "\n",
    "\n",
    "\n",
    "                        #cmap = cm.get_cmap('tab20c', 2)\n",
    "\n",
    "                        #colr = cmap.colors\n",
    "\n",
    "\n",
    "\n",
    "                        labels = [feature+\" Before\",feature+ \" After\"]\n",
    "\n",
    "                   # print(\"FINAL date:\",comparison_splitdate)    \n",
    "\n",
    "                    break\n",
    "\n",
    "                except:\n",
    "\n",
    "                    #display(df)\n",
    "\n",
    "                    comparison_splitdate_todate = datetime.strptime(comparison_splitdate, '%Y-%m-%d')+timedelta(days=1)\n",
    "\n",
    "                    comparison_splitdate = datetime.strftime(comparison_splitdate_todate,'%Y-%m-%d')\n",
    "\n",
    "                   # print(\"TRY:\",comparison_splitdate)\n",
    "\n",
    "                    #raise Exception(\"Date not found in dataframe\")\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    if feature == 'avg_shortest_path_length':\n",
    "\n",
    "        trend_line = False\n",
    "\n",
    "        for i in range(len(y)):\n",
    "\n",
    "            if type(y[i]) is str:\n",
    "\n",
    "                y[i] = None\n",
    "\n",
    "        if comparison:\n",
    "\n",
    "            if type(comparison_splitdate) is str:\n",
    "\n",
    "                for i in range(len(y2)):\n",
    "\n",
    "                    if type(y2[i]) is str:\n",
    "\n",
    "                        y2[i] = None         \n",
    "\n",
    "            if type(comparison_splitdate) is tuple:   \n",
    "\n",
    "                for i in range(len(y2)):\n",
    "\n",
    "                    if type(y2[i]) is str:\n",
    "\n",
    "                        y2[i] = None \n",
    "\n",
    "                for i in range(len(y3)):\n",
    "\n",
    "                    if type(y3[i]) is str: \n",
    "\n",
    "                        y3[i] = None\n",
    "\n",
    "    if trend_line:                    \n",
    "\n",
    "        #xd = x_init#mdates.date2num(list(pd.to_datetime(x_init)))\n",
    "\n",
    "        z = np.polyfit(xd,y_init,1)\n",
    "\n",
    "        p = np.poly1d(z)\n",
    "\n",
    "        sns.lineplot(x= x_init,y = p(xd),color='black',linewidth = 3, label='trend',linestyle=\"dashed\")\n",
    "\n",
    "\n",
    "\n",
    "    sns.set(rc={'figure.figsize':(30,15)}, style=\"white\", font_scale=2.5)\n",
    "\n",
    "    plot  = sns.lineplot(x=x,y=y, linewidth = 3, color=colr[0], label = labels[0])\n",
    "\n",
    "    \n",
    "\n",
    "    if comparison:\n",
    "\n",
    "        if type(comparison_splitdate) is str:\n",
    "\n",
    "            sns.lineplot(x=x2,y=y2, linewidth = 3, color=colr[1], label=labels[1])\n",
    "\n",
    "        if type(comparison_splitdate) is tuple:\n",
    "\n",
    "            sns.lineplot(x=x2,y=y2, linewidth = 3, color=colr[1], label=labels[1])\n",
    "\n",
    "            sns.lineplot(x=x3,y=y3, linewidth = 3, color=colr[2], label=labels[2])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    plot.set_title(name+\" - Distribution over time - \"+feature)\n",
    "\n",
    "    plot.set_ylabel(feature)\n",
    "\n",
    "    plot.set_xlabel(\"Date\")\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    if len(x) >100:\n",
    "\n",
    "        for ind, label in enumerate(plot.get_xticklabels()):\n",
    "\n",
    "            if ind % 3 == 0:  # every 10th label is kept\n",
    "\n",
    "                label.set_visible(True)\n",
    "\n",
    "            else:\n",
    "\n",
    "                label.set_visible(False)\n",
    "\n",
    "  #  for ind, label in enumerate(plot.get_yticklabels()):\n",
    "\n",
    "  #      if ind % 3 == 0:  # every 10th label is kept\n",
    "\n",
    "  #          label.set_visible(True)\n",
    "\n",
    "  #      else:\n",
    "\n",
    "  #          label.set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "    plot.tick_params(axis='x', rotation=90, labelsize=\"xx-small\")\n",
    "\n",
    "    if save: \n",
    "\n",
    "        if 'daily' not in os.listdir(ROOT_PATH):\n",
    "\n",
    "            os.mkdir(ROOT_PATH+'daily')\n",
    "\n",
    "            os.mkdir(ROOT_PATH+'daily/covid19/')\n",
    "\n",
    "        if 'covid19' not in os.listdir(ROOT_PATH+'daily/'):\n",
    "\n",
    "            os.mkdir(ROOT_PATH+'daily/covid19/')  \n",
    "\n",
    "                     \n",
    "\n",
    "        figure = plot.get_figure() \n",
    "\n",
    "        figure.savefig(f'{ROOT_PATH}/daily/covid19/{name}-{save_name}.png')\n",
    "\n",
    "   #     plt.clf()\n",
    "\n",
    "        return plot\n",
    "\n",
    "    \n",
    "\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6395be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_stats(Graph_dict):\n",
    "\n",
    "    df_stats = dict() \n",
    "\n",
    "    for dt in sorted(list(Graph_dict.keys())):\n",
    "\n",
    "        for g_name in list(Graph_dict[dt].keys()):\n",
    "\n",
    "            print(g_name,dt)\n",
    "\n",
    "            df_stats[dt+\"_\"+g_name] = compute_graph_stats(Graph_dict[dt][g_name], name=dt+\"_\"+g_name, str_to_write=\"\")\n",
    "\n",
    "    df = pd.DataFrame.from_dict(df_stats, orient='index')\n",
    "\n",
    "    df.assortativity = df.assortativity.astype(float)\n",
    "\n",
    "    df.drop('string_to_write', axis=1,inplace =True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covid_plots(df1,split_date=\"\", path_dirs=\"\", df_name = \"\",  comparison = False,skip_instr = [], merge = False):\n",
    "\n",
    "    important_features = ['mean','avg_shortest_path_length','transitivity','avg_clustering', 'assortativity', 'density']\n",
    "\n",
    "    df_init = df1 \n",
    "\n",
    "    \n",
    "\n",
    "    path = ROOT_PATH+ path_dirs \n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    if merge:\n",
    "\n",
    "        list_of_split = list(set([i.split('_')[1] for i in df_init.index]))\n",
    "\n",
    "    else:\n",
    "\n",
    "        list_of_split = list(set([i.split('_')[1]+\"_\"+i.split(\"_\")[2] for i in df_init.index]))\n",
    "\n",
    "    for instr in list_of_split:\n",
    "\n",
    "        if instr in skip_instr:\n",
    "\n",
    "            continue\n",
    "\n",
    "        print(instr)\n",
    "\n",
    "        df = df_init[df_init.index.str.contains(instr)]\n",
    "\n",
    "      \n",
    "\n",
    "  \n",
    "\n",
    "        plots = []\n",
    "\n",
    "        \n",
    "\n",
    "        if comparison:\n",
    "\n",
    "            comparison_splitdate = split_date #df2[df2.index.str.contains(instr)].index[0].split(\"_\")[0]\n",
    "\n",
    "        else:\n",
    "\n",
    "            comparison_splitdate = None\n",
    "\n",
    "        for imp in important_features:\n",
    "\n",
    "\n",
    "\n",
    "            plots.append(plot_stats_distr(df, imp, name =df_name,comparison =comparison, comparison_splitdate=comparison_splitdate, save =False))\n",
    "\n",
    "            plt.close()\n",
    "\n",
    " \n",
    "\n",
    "                     \n",
    "\n",
    "        with PdfPages(f'{path}{df_name}_{instr}.pdf') as pdf:\n",
    "\n",
    "            for p in plots:      \n",
    "\n",
    "                fig=p.get_figure()\n",
    "\n",
    "                pdf.savefig(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4713492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [('2018-05-02','2018-12-31'),('2019-01-01','2019-12-31'),('2020-01-01','2020-12-31'),('2021-01-01','2021-07-31')]\n",
    "\n",
    "ind = ['2018', '2019', '2020', '2021']\n",
    "\n",
    "for d in dates:\n",
    "\n",
    "    df_d = get_data(daterange=d,mode ='bic')\n",
    "\n",
    "    daily_graph_dict = get_daily_graph(df_d, mode =\"bic\")\n",
    "\n",
    "    stats_ext = get_df_stats(daily_graph_dict, get_dict= True)\n",
    "\n",
    "    get_covid_plots(stats_ext,  path_dirs=\"daily/external/\"+ind[dates.index(d)]+\"/\", df_name=\"external-agg\")\n",
    "\n",
    "    #get_ext_int_plots(stats_ext,  path_dirs=\"daily/external/\"+ind[dates.index(d)]+\"/\", df_name=\"external-agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c47e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [('2018-05-02','2018-12-31'),('2019-01-01','2019-12-31'),('2020-01-01','2020-12-31'),('2021-01-01','2021-07-31')]\n",
    "\n",
    "ind = ['2018', '2019', '2020', '2021']\n",
    "\n",
    "\n",
    "\n",
    "for d in dates:\n",
    "\n",
    "    path = ROOT_PATH+\"daily/external-agg/\"+ind[dates.index(d)]+\"/\"\n",
    "\n",
    "    df_d = get_data(daterange=d,mode ='bic')\n",
    "\n",
    "    daily_graph_dict = get_daily_graph(df_d, mode =\"bic\",ea = False, fi = True, sa= True)\n",
    "\n",
    "    stats_ext, stats_ext_dict = get_df_stats(daily_graph_dict, get_dict=True)\n",
    "\n",
    "    get_covid_plots(stats_ext,  path_dirs=path, df_name=\"external-agg\", merge = True)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    df_stats = dict() \n",
    "\n",
    "    os.makedirs(path+\"stats/\",exist_ok=True)\n",
    "\n",
    "    file = open(path+\"stats/\"+ind[dates.index(d)]+\".txt\", \"w\") \n",
    "\n",
    "    str_to_write_merged = \"\"\n",
    "\n",
    "    \n",
    "\n",
    "    for key in list(stats_ext_dict.keys()):    \n",
    "\n",
    "        str_to_write_merged += \"\\n\\n============ \"+key+\" ============\\n\"\n",
    "\n",
    "        str_to_write_merged+=  stats_ext_dict[key]['string_to_write']\n",
    "\n",
    "\n",
    "\n",
    "        str_to_write_merged+=\"\\n\"\n",
    "\n",
    "    \n",
    "\n",
    "    file.write(str_to_write_merged)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98f7ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790f2da",
   "metadata": {},
   "source": [
    "# **Disruptive Events**\n",
    "\n",
    "Disruptive Events could reversely change the Network topology and structure. Studies on disruptive events such as the September 11th 2001 terrorist attack have shown how this event can alter economics systems.Financial system of the USA have been disrupted, this affected the structure of interbank payments. This is due to the fact that the attacks damaged property and communications systems making impossible for many bank to execute payments. A similar study is performed taking into consideration two different case-studies of disruptive events:\n",
    "\n",
    "- The impact of Covid19 on the network metrics and topology during the first lockdown (January 2020 - June 2020)\n",
    "- The impact of large emission of Government Bonds such as BTP Italia and BTP Futura on the network metric and topology. The analysis takes into consideration the next ten days after the BTP announcement. This is considered a disruptive event since large amount of instructions are exchanged during the emission dates.\n",
    "\n",
    "## **Case study: Impact of Covid19**\n",
    "\n",
    "The impact of Covid19 on the network metrics and topology during the first lockdown (January 2020 - June 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df_before = get_data(daterange=('2019-01-01','2020-01-31'))\n",
    "\n",
    "covid_df_after = get_data(daterange=('2020-02-01','2020-12-31'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2703739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_graph(daily_df):\n",
    "\n",
    "    G_dict = dict()\n",
    "\n",
    "    for dt in list(daily_df['dt_business'].unique()):\n",
    "\n",
    "        G_dict[dt] = dict()\n",
    "\n",
    "\n",
    "\n",
    "        df = daily_df[daily_df['dt_business'] == dt]\n",
    "\n",
    "        G_dict[dt] = create_graph(df,external_agg=True,financial_instrument_agg=False,sett_agg = False)\n",
    "\n",
    "        G_dict_ETF = create_graph(df[df['ind_etf_mkt'] == 1],external_agg=True,financial_instrument_agg=True,sett_agg = False)\n",
    "\n",
    "        G_dict_ETF['ETF_N'] = G_dict_ETF.pop(\"N\")\n",
    "\n",
    "        try :\n",
    "\n",
    "           # print(G_dict_ETF)\n",
    "\n",
    "            G_dict_ETF['ETF_S'] = G_dict_ETF.pop(\"S\")\n",
    "\n",
    "        except KeyError:\n",
    "\n",
    "            print(\"ETF_S not avaiable in date:\", dt)\n",
    "\n",
    "            #display(df)\n",
    "\n",
    "            x = df\n",
    "\n",
    "        G_dict[dt] = {**G_dict_ETF, **G_dict[dt]}\n",
    "\n",
    "    return G_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35593fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_covid_bef = get_daily_graph(covid_df_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f5a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_covid_aft = get_daily_graph(covid_df_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f4537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_stats(Graph_dict,get_dict=False):\n",
    "\n",
    "    df_stats = dict() \n",
    "\n",
    "    for dt in sorted(list(Graph_dict.keys())):\n",
    "\n",
    "        for g_name in list(Graph_dict[dt].keys()):\n",
    "\n",
    "            df_stats[dt+\"_\"+g_name] = compute_graph_stats(Graph_dict[dt][g_name], name=dt+\"_\"+g_name, str_to_write=\"\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    df = pd.DataFrame.from_dict(df_stats, orient='index')\n",
    "\n",
    "    df.assortativity = df.assortativity.astype(float)\n",
    "\n",
    "    df.drop('string_to_write', axis=1,inplace =True)\n",
    "\n",
    "    if get_dict:\n",
    "\n",
    "        return  df,df_stats\n",
    "\n",
    "    else:\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_bef_stats = get_df_stats(G_covid_bef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_aft_stats = get_df_stats(G_covid_aft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28a2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_bef_stats[covid_bef_stats.index.str.contains('2019-07-01')].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_timeseries_stats(df, dates = True, sett = False, feature = 'mean'):\n",
    "\n",
    "    ensemble_dict = dict()\n",
    "\n",
    "    if dates and not sett:\n",
    "\n",
    "        for dt in set([i.split(\"_\")[0] for i in df.index]):\n",
    "\n",
    "            ensemble_dict[dt] = df[df.index.str.contains(dt)].mean()[feature]\n",
    "\n",
    "    if dates and sett:\n",
    "\n",
    "        sett = ['S', 'N']\n",
    "\n",
    "        for st in sett:\n",
    "\n",
    "            ensemble_dict[st] = dict()\n",
    "\n",
    "            for dt in  set([i.split(\"_\")[0] for i in df.index]):\n",
    "\n",
    "                ensemble_dict[st][dt] = df[(df.index.str.contains(dt)) & (df.index.str.contains(st))].mean()[feature] \n",
    "\n",
    "    return ensemble_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trend_line(df,df2, feature=\"degree\", name = \"\"):\n",
    "\n",
    "   # plt.figure(figsize=(12,8)) \n",
    "\n",
    "  \n",
    "\n",
    "   # plt.title('Network')\n",
    "\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    df2.index = pd.to_datetime(df2.index)\n",
    "\n",
    "    \n",
    "\n",
    "    y = df[feature].tolist()\n",
    "\n",
    "    x = mdates.date2num(list(pd.to_datetime(df.index)))\n",
    "\n",
    "    z = np.polyfit(x,y,1)\n",
    "\n",
    "    p = np.poly1d(z)\n",
    "\n",
    "    \n",
    "\n",
    "    y2 = df2[feature].tolist()\n",
    "\n",
    "    x2 = mdates.date2num(list(pd.to_datetime(df2.index)))\n",
    "\n",
    "    z2 = np.polyfit(x2,y2,1)\n",
    "\n",
    "    p2 = np.poly1d(z2)\n",
    "\n",
    "   # plt.plot(df.index, p(x), \"r--\", label=\"degree\")\n",
    "\n",
    "   # plt.plot(df.index, y, label = \"trend\")\n",
    "\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(15,12))\n",
    "\n",
    "    sns.set(style=\"white\", font_scale=1.5)\n",
    "\n",
    "    plt.title(\"Mean Degree over time\")\n",
    "\n",
    "   # plt.plot(deg,cnt,\"ro-\") # degree\n",
    "\n",
    "    plt.legend(['Dates'])\n",
    "\n",
    "    plt.xlabel('Dates')\n",
    "\n",
    "    plt.ylabel(feature)\n",
    "\n",
    "    \n",
    "\n",
    "    ax = sns.lineplot(x = df.index, y = y, label=feature+\" before\")\n",
    "\n",
    "    ax = sns.lineplot(x = df2.index, y = y2, label=feature+\" after\") \n",
    "\n",
    "    ax =sns.lineplot(x =  df.index, y = p(x), color='green', label=\"trend before\", linestyle=\"dashed\")\n",
    "\n",
    "    ax =sns.lineplot(x =  df2.index, y = p2(x2), color='red', label=\"trend after\", linestyle=\"dashed\")\n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n",
    "\n",
    "              ncol=1, fancybox=True, shadow=True )\n",
    "\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "    ax.set_title(\"Trend \"+feature+name+\" over time\")\n",
    "\n",
    "    \n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89549ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trends(feature, d1,d2, sett = False):\n",
    "\n",
    "    plots = []\n",
    "\n",
    "    path = ROOT_PATH+'daily/covid19/ensemble_trends/'\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    if sett == 'separated':\n",
    "\n",
    "        st = [True]\n",
    "\n",
    "    if sett == 'merge':\n",
    "\n",
    "        st = [False]\n",
    "\n",
    "    if sett == 'both':\n",
    "\n",
    "        st = [True,False]\n",
    "\n",
    "        \n",
    "\n",
    "    for s in st:\n",
    "\n",
    "        for feat in feature:\n",
    "\n",
    "            bef_avg_stats_dict = avg_timeseries_stats(covid_bef_stats, sett = s,feature=feat)\n",
    "\n",
    "            aft_avg_stats_dict = avg_timeseries_stats(covid_aft_stats, sett =s, feature=feat)\n",
    "\n",
    "\n",
    "\n",
    "            if s:\n",
    "\n",
    "                for type_sett in ['S','N']:\n",
    "\n",
    "                    df1_sett =   pd.DataFrame.from_dict(bef_avg_stats_dict[type_sett], columns=[feat], orient=\"index\")\n",
    "\n",
    "                    df2_sett = pd.DataFrame.from_dict( aft_avg_stats_dict[type_sett], columns=[feat], orient=\"index\")\n",
    "\n",
    "                    plots.append(plot_trend_line(df1_sett,df2_sett,name = \"_\"+type_sett, feature=feat))\n",
    "\n",
    "            else:\n",
    "\n",
    "                df1  = pd.DataFrame.from_dict(avg_timeseries_stats(covid_bef_stats,feature=feat), columns=[feat], orient=\"index\")\n",
    "\n",
    "                df2 = pd.DataFrame.from_dict(avg_timeseries_stats(covid_aft_stats, feature = feat), columns=[feat], orient=\"index\")\n",
    "\n",
    "                plots.append(plot_trend_line(df1,df2, feat))\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Saving PDFs\")\n",
    "\n",
    "    with PdfPages(f'{path}/trends.pdf') as pdf:\n",
    "\n",
    "            for p in plots:      \n",
    "\n",
    "                fig=p.get_figure()\n",
    "\n",
    "                pdf.savefig(fig,bbox_inches='tight')\n",
    "\n",
    "    return plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58002ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trends(['mean','assortativity','transitivity','std','avg_clustering','density'],covid_bef_stats,covid_aft_stats, sett='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd0846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats_distr(df,feature, comparison =False, comparison_splitdate = None, name=\"\",  save = False,save_name =\"\", trend_line=True):\n",
    "\n",
    "    x = [i.split(\"_\")[0] for i in list(df.index)]\n",
    "\n",
    "    y = list(df[feature])\n",
    "\n",
    "    x_init = x\n",
    "\n",
    "    xd = mdates.date2num(list(pd.to_datetime(x)))\n",
    "\n",
    "    y_init = y\n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    " \n",
    "\n",
    "    labels = [feature]\n",
    "\n",
    "#     cmap = cm.get_cmap('tab20c', 1)\n",
    "\n",
    "    colr = ['blue','red','green'] #[cmap.colors[0]]\n",
    "\n",
    "    label2 = None\n",
    "\n",
    "    if comparison:\n",
    "\n",
    "        if comparison_splitdate is None:\n",
    "\n",
    "            raise Expception(\"If you want to compare before and after, please specify the comparison_splitdate parameter\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            while True:\n",
    "\n",
    "                try:\n",
    "\n",
    "               #     print(comparison_splitdate)\n",
    "\n",
    "               #     print(type(comparison_splitdate))\n",
    "\n",
    "                    if type(comparison_splitdate) is tuple:\n",
    "\n",
    "                      \n",
    "\n",
    "                        index = [x.index(i) for i in comparison_splitdate]\n",
    "\n",
    "                        part1 = index[0]\n",
    "\n",
    "                        part2 = index[1]\n",
    "\n",
    "        \n",
    "\n",
    "                        x2 = x[part1:part2+1]\n",
    "\n",
    "                        x3 = x[part2:] \n",
    "\n",
    "                        x = x[:part1+1]\n",
    "\n",
    "\n",
    "\n",
    "                        y2 = y[part1:part2+1]\n",
    "\n",
    "                        y3 = y[part2:]\n",
    "\n",
    "                        y = y[:part1+1]\n",
    "\n",
    "                    \n",
    "\n",
    "                        \n",
    "\n",
    "                        #cmap = cm.get_cmap('tab20c', len(index)+1)\n",
    "\n",
    "                        #colr = cmap.colors\n",
    "\n",
    "                        labels = [feature+\" Before\",feature+\" BTP Emission\",feature+ \" After\"]\n",
    "\n",
    "                    if type(comparison_splitdate) is str:\n",
    "\n",
    "                        index= x.index(comparison_splitdate)\n",
    "\n",
    "                        x2 = x[index:]\n",
    "\n",
    "                        x = x[:index+1]\n",
    "\n",
    "                        y2 = y[index:]\n",
    "\n",
    "                        y = y[:index+1]\n",
    "\n",
    "\n",
    "\n",
    "                        #cmap = cm.get_cmap('tab20c', 2)\n",
    "\n",
    "                        #colr = cmap.colors\n",
    "\n",
    "\n",
    "\n",
    "                        labels = [feature+\" Before\",feature+ \" After\"]\n",
    "\n",
    "                   # print(\"FINAL date:\",comparison_splitdate)    \n",
    "\n",
    "                    break\n",
    "\n",
    "                except:\n",
    "\n",
    "                    #display(df)\n",
    "\n",
    "                    comparison_splitdate_todate = datetime.strptime(comparison_splitdate, '%Y-%m-%d')+timedelta(days=1)\n",
    "\n",
    "                    comparison_splitdate = datetime.strftime(comparison_splitdate_todate,'%Y-%m-%d')\n",
    "\n",
    "                   # print(\"TRY:\",comparison_splitdate)\n",
    "\n",
    "                    #raise Exception(\"Date not found in dataframe\")\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    if feature == 'avg_shortest_path_length':\n",
    "\n",
    "        trend_line = False\n",
    "\n",
    "        for i in range(len(y)):\n",
    "\n",
    "            if type(y[i]) is str:\n",
    "\n",
    "                y[i] = None\n",
    "\n",
    "        if comparison:\n",
    "\n",
    "            if type(comparison_splitdate) is str:\n",
    "\n",
    "                for i in range(len(y2)):\n",
    "\n",
    "                    if type(y2[i]) is str:\n",
    "\n",
    "                        y2[i] = None         \n",
    "\n",
    "            if type(comparison_splitdate) is tuple:   \n",
    "\n",
    "                for i in range(len(y2)):\n",
    "\n",
    "                    if type(y2[i]) is str:\n",
    "\n",
    "                        y2[i] = None \n",
    "\n",
    "                for i in range(len(y3)):\n",
    "\n",
    "                    if type(y3[i]) is str: \n",
    "\n",
    "                        y3[i] = None\n",
    "\n",
    "    if trend_line:                    \n",
    "\n",
    "        #xd = x_init#mdates.date2num(list(pd.to_datetime(x_init)))\n",
    "\n",
    "        z = np.polyfit(xd,y_init,1)\n",
    "\n",
    "        p = np.poly1d(z)\n",
    "\n",
    "        sns.lineplot(x= x_init,y = p(xd),color='black',linewidth = 3, label='trend',linestyle=\"dashed\")\n",
    "\n",
    "\n",
    "\n",
    "    sns.set(rc={'figure.figsize':(30,15)}, style=\"white\", font_scale=2.5)\n",
    "\n",
    "    plot  = sns.lineplot(x=x,y=y, linewidth = 3, color=colr[0], label = labels[0])\n",
    "\n",
    "    \n",
    "\n",
    "    if comparison:\n",
    "\n",
    "        if type(comparison_splitdate) is str:\n",
    "\n",
    "            sns.lineplot(x=x2,y=y2, linewidth = 3, color=colr[1], label=labels[1])\n",
    "\n",
    "        if type(comparison_splitdate) is tuple:\n",
    "\n",
    "            sns.lineplot(x=x2,y=y2, linewidth = 3, color=colr[1], label=labels[1])\n",
    "\n",
    "            sns.lineplot(x=x3,y=y3, linewidth = 3, color=colr[2], label=labels[2])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    plot.set_title(name+\" - Distribution over time - \"+feature)\n",
    "\n",
    "    plot.set_ylabel(feature)\n",
    "\n",
    "    plot.set_xlabel(\"Date\")\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    if len(x) >100:\n",
    "\n",
    "        for ind, label in enumerate(plot.get_xticklabels()):\n",
    "\n",
    "            if ind % 3 == 0:  # every 10th label is kept\n",
    "\n",
    "                label.set_visible(True)\n",
    "\n",
    "            else:\n",
    "\n",
    "                label.set_visible(False)\n",
    "\n",
    "  #  for ind, label in enumerate(plot.get_yticklabels()):\n",
    "\n",
    "  #      if ind % 3 == 0:  # every 10th label is kept\n",
    "\n",
    "  #          label.set_visible(True)\n",
    "\n",
    "  #      else:\n",
    "\n",
    "  #          label.set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "    plot.tick_params(axis='x', rotation=90, labelsize=\"xx-small\")\n",
    "\n",
    "    if save: \n",
    "\n",
    "        if 'daily' not in os.listdir(ROOT_PATH):\n",
    "\n",
    "            os.mkdir(ROOT_PATH+'daily')\n",
    "\n",
    "            os.mkdir(ROOT_PATH+'daily/covid19/')\n",
    "\n",
    "        if 'covid19' not in os.listdir(ROOT_PATH+'daily/'):\n",
    "\n",
    "            os.mkdir(ROOT_PATH+'daily/covid19/')  \n",
    "\n",
    "                     \n",
    "\n",
    "        figure = plot.get_figure() \n",
    "\n",
    "        figure.savefig(f'{ROOT_PATH}/daily/covid19/{name}-{save_name}.png')\n",
    "\n",
    "   #     plt.clf()\n",
    "\n",
    "        return plot\n",
    "\n",
    "    \n",
    "\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6cd524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covid_plots(df1,split_date=\"\", path_dirs=\"\", df_name = \"\",  comparison = False,skip_instr = []):\n",
    "\n",
    "    important_features = ['mean','avg_shortest_path_length','transitivity','avg_clustering', 'assortativity', 'density']\n",
    "\n",
    "    df_init = df1 \n",
    "\n",
    "    \n",
    "\n",
    "    path = ROOT_PATH+ path_dirs \n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "    for instr in list(set([i.split('_')[1]+\"_\"+i.split(\"_\")[2] for i in df_init.index])):\n",
    "\n",
    "        if instr in skip_instr:\n",
    "\n",
    "            continue\n",
    "\n",
    "        print(instr)\n",
    "\n",
    "        df = df_init[df_init.index.str.contains(instr)]\n",
    "\n",
    "      \n",
    "\n",
    "  \n",
    "\n",
    "        plots = []\n",
    "\n",
    "        \n",
    "\n",
    "        if comparison:\n",
    "\n",
    "            comparison_splitdate = split_date #df2[df2.index.str.contains(instr)].index[0].split(\"_\")[0]\n",
    "\n",
    "        else:\n",
    "\n",
    "            comparison_splitdate = None\n",
    "\n",
    "        for imp in important_features:\n",
    "\n",
    "\n",
    "\n",
    "            plots.append(plot_stats_distr(df, imp, name =df_name,comparison =comparison, comparison_splitdate=comparison_splitdate, save =False))\n",
    "\n",
    "            plt.close()\n",
    "\n",
    " \n",
    "\n",
    "                     \n",
    "\n",
    "        with PdfPages(f'{path}{df_name}_{instr}.pdf') as pdf:\n",
    "\n",
    "            for p in plots:      \n",
    "\n",
    "                fig=p.get_figure()\n",
    "\n",
    "                pdf.savefig(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695551b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_covid_plots(covid_bef_stats,  path_dirs=\"daily/covid19/before/\", df_name=\"before\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a26788",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_covid_plots(covid_aft_stats, path_dirs=\"daily/covid19/after/\", df_name=\"after\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcca09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_covid_plots(df1=pd.concat([covid_bef_stats,covid_aft_stats]), split_date= covid_aft_stats[covid_aft_stats.index.str.contains(\"ETF\")].index[0].split(\"_\")[0], path_dirs=\"daily/covid19/comparison/\", comparison=True,df_name=\"merge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be60a5f",
   "metadata": {},
   "source": [
    "## **Case study: BTP Italia and BTP Futura**\n",
    "\n",
    "The impact of large emission of Government Bonds such as BTP Italia and BTP Futura on the network metric and topology. The analysis takes into consideration the next ten days after the BTP announcement. This is considered a disruptive event since large amount of instructions are exchanged during the emission dates.\n",
    "\n",
    "BTP Italia Emissions:\n",
    "\n",
    "- dal 14 al 17 maggio 2018\n",
    "- dal 19 al 22 novembre 2018\n",
    "- dal 21 al 23 ottobre 2019\n",
    "- dal 18 al 20 maggio 2021\n",
    "\n",
    "BTP Futura Emissions:\n",
    "\n",
    "- dal 6 al 10 luglio 2020\n",
    "- dal 9 al 13 novembre 2020\n",
    "- dal 19 al 23 aprile 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c980014",
   "metadata": {},
   "outputs": [],
   "source": [
    "BTP_italia_dates = {\n",
    "\n",
    "    \"May2018\":('2018-05-14','2018-05-17'),\n",
    "\n",
    "    \"Nov2018\":('2018-11-19', '2018-11-22'),\n",
    "\n",
    "    \"Oct2019\":('2019-10-21', '2019-10-23'),\n",
    "\n",
    "    \"May2021\":('2021-05-18','2021-05-20')\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "BTP_futura_dates = {\n",
    "\n",
    "    \"Jul2020\":('2020-07-06', '2020-07-10'),\n",
    "\n",
    "    \"Nov2020\":('2020-11-09', '2020-11-13'),\n",
    "\n",
    "    \"Apr2021\":('2021-04-19', '2021-04-23')\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a00442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_5day_diff(date,type_diff,end_date = None, to_string = True):\n",
    "\n",
    "    days_num = 0\n",
    "\n",
    "    i = 0 \n",
    "\n",
    "    days_list = []\n",
    "\n",
    "    if end_date is None:\n",
    "\n",
    "        flag_date = 5\n",
    "\n",
    "        end_date = date+timedelta(days=10000)\n",
    "\n",
    "    else:\n",
    "\n",
    "        flag_date = (end_date-date).days-1\n",
    "\n",
    "    while len(days_list) < flag_date and date <= end_date :\n",
    "\n",
    "\n",
    "\n",
    "        if date.weekday() < 5: \n",
    "\n",
    "            days_list.append(date)\n",
    "\n",
    "        if type_diff == \"before\":\n",
    "\n",
    "            date = date-timedelta(days=1)\n",
    "\n",
    "        if type_diff == \"after\":\n",
    "\n",
    "            date = date+timedelta(days=1)\n",
    "\n",
    "        i+= 1\n",
    "\n",
    "    if to_string:\n",
    "\n",
    "        return [datetime.strftime(i,'%Y-%m-%d') for i in days_list]\n",
    "\n",
    "    return days_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd68bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_dates(daterange):\n",
    "\n",
    "\n",
    "\n",
    "    start = daterange[0]\n",
    "\n",
    "    end = daterange[1]\n",
    "\n",
    "  \n",
    "\n",
    "    start_date = datetime.strptime(start, '%Y-%m-%d')\n",
    "\n",
    "    end_date = datetime.strptime(end, '%Y-%m-%d')\n",
    "\n",
    "\n",
    "\n",
    "    date_of_interest = dict()\n",
    "\n",
    "\n",
    "\n",
    "    date_of_interest['before'] = get_5day_diff(start_date, type_diff=\"before\") \n",
    "\n",
    "    date_of_interest['after'] = get_5day_diff(end_date, type_diff=\"after\")\n",
    "\n",
    "    date_of_interest['critical'] = get_5day_diff(start_date, end_date=end_date, type_diff=\"after\")\n",
    "\n",
    "    date_of_interest['all'] = sorted(date_of_interest['before']+ date_of_interest['critical']+ date_of_interest['after'])\n",
    "\n",
    "    return date_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04722444",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_dict = get_nearest_dates(BTP_italia_dates['Nov2018'])\n",
    "dates_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BTP_italia_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_btp_dict = dict()\n",
    "\n",
    "btp_stats_dict = dict()\n",
    "\n",
    "\n",
    "\n",
    "all_btp_dict = dict()\n",
    "\n",
    "for period in BTP_italia_dates:\n",
    "\n",
    "    print(period)\n",
    "\n",
    "    dates_dict = get_nearest_dates(BTP_italia_dates[period])\n",
    "\n",
    "    btp_df = get_data(daterange=(dates_dict['all'][0],dates_dict['all'][-1]))\n",
    "\n",
    "    G_btp_dict[period] = get_daily_graph(btp_df)\n",
    "\n",
    "    btp_stats_dict[period] = get_df_stats(G_btp_dict[period])\n",
    "\n",
    "    \n",
    "\n",
    "    skips = []\n",
    "\n",
    "    for instr in set([i.split(\"_\")[1]+\"_\"+i.split(\"_\")[2] for i in btp_stats_dict[period].index]):\n",
    "\n",
    "        if dates_dict['critical'][0] not in [i.split(\"_\")[0] for i in btp_stats_dict[period][btp_stats_dict[period].index.str.contains(instr)].index]:\n",
    "\n",
    "            skips.append(instr)\n",
    "\n",
    "    get_covid_plots(btp_stats_dict[period], (dates_dict['critical'][0],dates_dict['after'][0]),path_dirs='daily/btp/btp-italia/'+period+'/', comparison=True, df_name = \"btp-italia\", skip_instr = skips)\n",
    "\n",
    "\n",
    "\n",
    "all_btp_dict['ITALIA'] = {\"G\":G_btp_dict, \"stat\":btp_stats_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab65a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_btp_dict = dict()\n",
    "\n",
    "btp_stats_dict = dict()\n",
    "\n",
    "\n",
    "\n",
    "for period in BTP_futura_dates:\n",
    "\n",
    "    print(period)\n",
    "\n",
    "    dates_dict = get_nearest_dates(BTP_futura_dates[period])\n",
    "\n",
    "    btp_df = get_data(daterange=(dates_dict['all'][0],dates_dict['all'][-1]))\n",
    "\n",
    "    G_btp_dict[period] = get_daily_graph(btp_df)\n",
    "\n",
    "    btp_stats_dict[period] = get_df_stats(G_btp_dict[period])\n",
    "\n",
    "    \n",
    "\n",
    "    skips = []\n",
    "\n",
    "    for instr in set([i.split(\"_\")[1]+\"_\"+i.split(\"_\")[2] for i in btp_stats_dict[period].index]):\n",
    "\n",
    "        if dates_dict['critical'][0] not in [i.split(\"_\")[0] for i in btp_stats_dict[period][btp_stats_dict[period].index.str.contains(instr)].index]:\n",
    "\n",
    "            skips.append(instr)\n",
    "\n",
    "    get_covid_plots(btp_stats_dict[period], (dates_dict['critical'][0],dates_dict['after'][0]),path_dirs='daily/btp/btp-futura/'+period+'/', comparison=True, df_name = \"btp-futura\", skip_instr = skips)\n",
    "\n",
    "    \n",
    "\n",
    "all_btp_dict['FUTURA'] = {\"G\":G_btp_dict, \"stat\":btp_stats_dict}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
