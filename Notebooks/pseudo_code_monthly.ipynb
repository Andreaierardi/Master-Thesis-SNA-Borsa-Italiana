{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Network Analysis on Post Trade data:**\n",
    "\n",
    "## **variation in topology and stability of the Scale-free Networks over time**\n",
    "\n",
    "The aim of the study is the construction of Social Networks from Settlement Instructions of T2S:\n",
    "\n",
    "- The topological structure of the Network may change over time due to disruptive events.\n",
    "- Two Case-studies are conducted on disruptive events: COVID19 and BTP Italia, BTP Futura emissions.\n",
    "- The identification over time of a Scale-free behavior and a ranking for the most central nodes is conducted.\n",
    "- Moreover, a networks resiliency analysis is performed using random and targeted attacks.\n",
    "\n",
    "****Library requirements****\n",
    "\n",
    "- Powerlaw\n",
    "- Jsonpickle\n",
    "- Pyvis\n",
    "- igraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import io\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(123456789)\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import time\n",
    "\n",
    "import math\n",
    "\n",
    "import json\n",
    "\n",
    "import gc\n",
    "\n",
    "import pickle\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator, FixedFormatter, FixedLocator\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "import boto3\n",
    "\n",
    "from s3 import S3\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from scipy.stats import kstest\n",
    "\n",
    "import powerlaw\n",
    "\n",
    "import pyvis\n",
    "\n",
    "from pyvis.network import Network\n",
    "\n",
    "import igraph as ig\n",
    "\n",
    "import leidenalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"anonnames.txt\")\n",
    "\n",
    "names = []\n",
    "\n",
    "for i in file:\n",
    "\n",
    "    names.append(i.split(\" \")[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "company_names = [ \"Telecom\",\n",
    "\n",
    "\"Software\",\n",
    "\n",
    "\"Technology\",\n",
    "\n",
    "\"Hardware\",\n",
    "\n",
    "\"Electronics\",\n",
    "\n",
    "\"Consulting\",\n",
    "\n",
    "\"General\",\n",
    "\n",
    "\"Frontier\",\n",
    "\n",
    "\"Alpha\",\n",
    "\n",
    "\"Industries\",\n",
    "\n",
    "\"Net\",\n",
    "\n",
    "\"People\",\n",
    "\n",
    "\"Star\",\n",
    "\n",
    "\"Bell\",\n",
    "\n",
    "\"Research\",\n",
    "\n",
    "\"Architecture\",\n",
    "\n",
    "\"Building\",\n",
    "\n",
    "\"Construction\",\n",
    "\n",
    "\"Medicine\",\n",
    "\n",
    "\"Hill\",\n",
    "\n",
    "\"Graphics\",\n",
    "\n",
    "\"Analysis\",\n",
    "\n",
    "\"Vision\",\n",
    "\n",
    "\"Contract\",\n",
    "\n",
    "\"Solutions\",\n",
    "\n",
    "\"Advanced\",\n",
    "\n",
    "\"Venture\",\n",
    "\n",
    "\"Innovation\",\n",
    "\n",
    "\"Systems\",\n",
    "\n",
    "\"Solutions\",\n",
    "\n",
    "\"Provider\",\n",
    "\n",
    "\"Design\",\n",
    "\n",
    "\"Internet\",\n",
    "\n",
    "\"Virtual\",\n",
    "\n",
    "\"Vision\",\n",
    "\n",
    "\"Application\",\n",
    "\n",
    "\"Signal\",\n",
    "\n",
    "\"Network\",\n",
    "\n",
    "\"Net\",\n",
    "\n",
    "\"Data\",\n",
    "\n",
    "\"Electronic\",\n",
    "\n",
    "\"Max\",\n",
    "\n",
    "\"Adventure\",\n",
    "\n",
    "\"Atlantic\",\n",
    "\n",
    "\"Pacific\",\n",
    "\n",
    "\"North\",\n",
    "\n",
    "\"East\",\n",
    "\n",
    "\"South\",\n",
    "\n",
    "\"West\",\n",
    "\n",
    "\"Speed\",\n",
    "\n",
    "\"Universal\",\n",
    "\n",
    "\"Galaxy\",\n",
    "\n",
    "\"Future\",\n",
    "\n",
    "\"Digital\",\n",
    "\n",
    "\"Studio\",\n",
    "\n",
    "\"Interactive\",\n",
    "\n",
    "\"Source\",\n",
    "\n",
    "\"Omega\",\n",
    "\n",
    "\"Direct\",\n",
    "\n",
    "\"Resource\",\n",
    "\n",
    "\"Power\",\n",
    "\n",
    "\"Federated\",\n",
    "\n",
    "\"Star\"]\n",
    "\n",
    "\n",
    "\n",
    "company_types = ('LawFirm', 'Generic', 'Short')\n",
    "\n",
    "last_names = names\n",
    "\n",
    "def create_company_name(biz_type=None):\n",
    "\n",
    "    name = []\n",
    "\n",
    "    if not biz_type:\n",
    "\n",
    "        biz_type = random.choice(company_types)\n",
    "\n",
    "    if biz_type == \"LawFirm\":\n",
    "\n",
    "        name.append( random.choice(last_names)+ \", \" + random.choice(last_names) + \" & \" + \n",
    "\n",
    "                     random.choice(last_names))\n",
    "\n",
    "        name.append('LLP')\n",
    "\n",
    "    else:\n",
    "\n",
    "        for i in range(1,random.randint(2,4)):\n",
    "\n",
    "            rand_name = random.choice(company_names)\n",
    "\n",
    "            if rand_name not in name:\n",
    "\n",
    "                name.append(rand_name)\n",
    "\n",
    "        if biz_type == 'Generic':\n",
    "\n",
    "            name.append(random.choice(company_types))\n",
    "\n",
    "        elif len(name) < 3:\n",
    "\n",
    "            name.append(random.choice(company_names))\n",
    "\n",
    "    return \" \".join(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Retrieval**\n",
    "\n",
    "The result plots and statistics will be stored in the dirsÂ **/home/ec2-user/SageMaker/Tesi_results/**\n",
    "\n",
    "- monthly subdir fs3_helper = S3(bucket_name='mt-res-prod-ml-bucket')or monthly data\n",
    "- daily subdir for daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_helper = S3(bucket_name='mt-res-prod-ml-bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the directory where the input data is\n",
    "\n",
    "\n",
    "\n",
    "bucket_name = 'mt-res-prod-ml-bucket'\n",
    "\n",
    "\n",
    "\n",
    "#output_path = f's3://{bucket_name}/tmp-andrea-spark'\n",
    "\n",
    "ROOT_PATH = '/home/ec2-user/SageMaker/Tesi_results/'\n",
    "\n",
    "if 'Tesi_results' not in os.listdir('/home/ec2-user/SageMaker/'):\n",
    "\n",
    "    os.mkdir('/home/ec2-user/SageMaker/Tesi_results')\n",
    "\n",
    "\n",
    "\n",
    "PATH_TO_INPUT_DIR=f'TESI/Tesi-Andrea/RAW'\n",
    "\n",
    "PATH_TO_DATASET = f'TESI/Tesi-Andrea/DATA'\n",
    "\n",
    "path_to_root_directory = f's3://{bucket_name}/{PATH_TO_DATASET}'\n",
    "\n",
    "\n",
    "\n",
    "# Define the partition column of the input directory\n",
    "\n",
    "PARTITION_COLS=['dt_business']\n",
    "\n",
    "START_DATE = '2018-04-01'\n",
    "\n",
    "#END_DATE = '2018-05-04'\n",
    "\n",
    "END_DATE = '2021-07-30'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "anonymize_data = True\n",
    "\n",
    "if anonymize_data:\n",
    "\n",
    "    ANONYMIZE_DICT = dict()\n",
    "\n",
    "    ROOT_PATH = ROOT_PATH+'anonymized/'\n",
    "\n",
    "    os.makedirs(ROOT_PATH, exist_ok=True)\n",
    "\n",
    "    if 'anonymize_companies.pkl' in os.listdir(ROOT_PATH):\n",
    "\n",
    "        with open(ROOT_PATH+'anonymize_companies.pkl', 'rb') as handle:\n",
    "\n",
    "            ANONYMIZE_DICT = pickle.load(handle)\n",
    "\n",
    "\n",
    "\n",
    "ROOT_PATH = ROOT_PATH+'monthly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_anonymized(node_list):\n",
    "\n",
    "    for i in node_list:\n",
    "\n",
    "        if i == 'MONTE TITOLI':\n",
    "\n",
    "            ANONYMIZE_DICT[i] = 'MONTE TITOLI'\n",
    "\n",
    "            continue\n",
    "\n",
    "        if i == 'C.COMP.GARANZIA':\n",
    "\n",
    "            ANONYMIZE_DICT[i] = 'C.COMP.GARANZIA'\n",
    "\n",
    "            continue\n",
    "\n",
    "        r_name =  create_company_name(\"Short\")\n",
    "\n",
    "        if r_name in ANONYMIZE_DICT.values():\n",
    "\n",
    "            flag = True\n",
    "\n",
    "            while flag :\n",
    "\n",
    "                r_name =  create_company_name(\"Short\")\n",
    "\n",
    "                if r_name not in ANONYMIZE_DICT.values():\n",
    "\n",
    "                    flag = False\n",
    "\n",
    "        if i not in ANONYMIZE_DICT.keys():\n",
    "\n",
    "            ANONYMIZE_DICT[i] = r_name\n",
    "\n",
    "    with open(ROOT_PATH+'anonymize_companies.pkl', 'wb') as handle:\n",
    "\n",
    "            pickle.dump(ANONYMIZE_DICT, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "\n",
    "    return  [ANONYMIZE_DICT[i] for i in node_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_single_plot(plot, path, name):\n",
    "\n",
    "    os.makedirs(ROOT_PATH+path,exist_ok = True)\n",
    "\n",
    "    fig=plot.get_figure()\n",
    "\n",
    "    fig.savefig(f'{ROOT_PATH}/{path}/{name}.png', bbox_inches='tight')\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"plot saved: \",f'{ROOT_PATH}/{path}/{name}.png' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data(forced = False):\n",
    "\n",
    "    try:\n",
    "\n",
    "        if forced:\n",
    "\n",
    "            return get_data(forced)\n",
    "\n",
    "        else:\n",
    "\n",
    "            alldata_df =  pd.read_csv(os.path.join(path_to_root_directory,'aggregated_daily.csv'), sep=',')\n",
    "\n",
    "            print(\"Data retrieved successfully: shape\",alldata_df.shape)\n",
    "\n",
    "            return alldata_df\n",
    "\n",
    "        \n",
    "\n",
    "    except:\n",
    "\n",
    "        print(\"NOT found\")\n",
    "\n",
    "        return get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(forced=False):\n",
    "\n",
    "    datelist = get_datelist(START_DATE,END_DATE)\n",
    "\n",
    "    if 'MONTHLY' in [i.key.split(\"/\")[-2] for i in s3_helper.bucket.objects.filter(Prefix=PATH_TO_DATASET)] and forced == False:\n",
    "\n",
    "        csv_list = [i for i in [i.key.split(\"/\")[-1] for i in s3_helper.bucket.objects.filter(Prefix=PATH_TO_DATASET+'/MONTHLY')]]\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for f in csv_list:\n",
    "\n",
    "            df = pd.concat([pd.read_csv(os.path.join(path_to_root_directory+'/MONTHLY',f)),df])\n",
    "\n",
    "        return df\n",
    "\n",
    "    else:\n",
    "\n",
    "        month_dict_df = dict() \n",
    "\n",
    "        for start,end in datelist:\n",
    "\n",
    "            print(\"\\n=============\\nDATE:\",start,\"--\",end)\n",
    "\n",
    "\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            print(\"Start reading dataset...\")\n",
    "\n",
    "            raw_df = read_input_dataset(start,end)\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            #raw_df[raw_df['cd_sett']== 'N']\n",
    "\n",
    "            print(\"Process time: \",round((end_time - start_time),2),\" seconds\")\n",
    "\n",
    "            raw_group = raw_df.groupby(['dt_business','ds_deli_pty1','ds_rece_pty1','cd_sett','cd_sec_at','ind_etf_mkt'])[['am_pend','am_amt']].sum()\n",
    "\n",
    "            raw_group = raw_group.reset_index()\n",
    "\n",
    "            month_dict_df[start+'_'+end]  =raw_group\n",
    "\n",
    "            print(raw_group.shape)\n",
    "\n",
    "            #raw_group.to_pickle(f'{start}_{end}.pkl')\n",
    "\n",
    "              \n",
    "\n",
    "            utils.upload_dataset_to_aws_s3_v2(df=raw_group, \\\n",
    "\n",
    "                                bucket='mt-res-prod-ml-bucket',\\\n",
    "\n",
    "                                prefix=PATH_TO_DATASET+'/MONTHLY',\\\n",
    "\n",
    "                                output_file_name=f'{start}_{end}.csv',\\\n",
    "\n",
    "                                index=False, header=True, sep=',', decimal='.')\n",
    "\n",
    "            print(\"UPLOADED to s3\", f'{start}_{end}.csv')\n",
    "\n",
    "            \n",
    "\n",
    "        alldata_df = pd.DataFrame()\n",
    "\n",
    "        for key in month_dict_df:\n",
    "\n",
    "            #print(key)\n",
    "\n",
    "            alldata_df = pd.concat([alldata_df, month_dict_df[key]])\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "        utils.upload_dataset_to_aws_s3_v2(df=alldata_df,\n",
    "\n",
    "                                bucket='mt-res-prod-ml-bucket',\n",
    "\n",
    "                                prefix=PATH_TO_DATASET,\n",
    "\n",
    "                                output_file_name='aggregated_daily.csv',\n",
    "\n",
    "                                index=False, header=True, sep=',', decimal='.')\n",
    "\n",
    "        \n",
    "\n",
    "        return alldata_df\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def read_input_dataset(START_DATE, END_DATE):\n",
    "\n",
    "    # Read the input .parquet dataset\n",
    "\n",
    "    raw_df = s3_helper.read_parquet(remote_dir=PATH_TO_INPUT_DIR, partition_cols=PARTITION_COLS, start_date =START_DATE,end_date=END_DATE)\n",
    "\n",
    "    \n",
    "\n",
    "    # Reset the index of the Pandas DataFrame\n",
    "\n",
    "    raw_df = raw_df.reset_index(drop=True)\n",
    "\n",
    "    \n",
    "\n",
    "    # convert column names to lowercase \n",
    "\n",
    "    raw_df.columns = [x.lower() for x in raw_df.columns]\n",
    "\n",
    "    \n",
    "\n",
    "    return raw_df\n",
    "\n",
    "\n",
    "\n",
    "def get_datelist(START_DATE, END_DATE):\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    datelist = []\n",
    "\n",
    "    d = datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "\n",
    "    nextmonthdate = d\n",
    "\n",
    "    while (nextmonthdate < datetime.strptime(END_DATE, '%Y-%m-%d')):\n",
    "\n",
    "            cnt+=1\n",
    "\n",
    "            #print(d.month+cnt)\n",
    "\n",
    "            #print(\"TRY\",nextmonthdate)\n",
    "\n",
    "            if nextmonthdate.month >= 12:\n",
    "\n",
    "                nextmonthdate = nextmonthdate.replace(year=nextmonthdate.year+1, month=1)\n",
    "\n",
    "            #    print(\"IF\",nextmonthdate)\n",
    "\n",
    "                end_month = nextmonthdate.replace(month=nextmonthdate.month+1)-timedelta(days=1)\n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                nextmonthdate = nextmonthdate.replace(month=nextmonthdate.month+1) \n",
    "\n",
    "                if nextmonthdate.month >= 12:\n",
    "\n",
    "                    end_month = nextmonthdate.replace(year=nextmonthdate.year+1, month=1)-timedelta(days=1)\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "\n",
    "                    end_month = nextmonthdate.replace(month=nextmonthdate.month+1)-timedelta(days=1)\n",
    "\n",
    "\n",
    "\n",
    "            #end_month = nextmonthdate.replace(month=nextmonthdate.month+1)+timedelta(days=1)\n",
    "\n",
    "            datelist.append((str(nextmonthdate).split(\" \")[0], str(end_month).split(\" \")[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    datelist.pop(-1)\n",
    "\n",
    "    return datelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retrieve_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-54728ca73bd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0malldata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'retrieve_data' is not defined"
     ]
    }
   ],
   "source": [
    "alldata_df = retrieve_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata_df_agg_monthly = alldata_df\n",
    "\n",
    "alldata_df_agg_monthly.index = pd.to_datetime(alldata_df_agg_monthly['dt_business'],format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata_df_agg_monthly = alldata_df_agg_monthly.groupby(by=['ds_deli_pty1','ds_rece_pty1','cd_sett','cd_sec_at','ind_etf_mkt',pd.Grouper(freq='M')])[['am_pend','am_amt']].sum().reset_index().sort_values(by='dt_business')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly = dict()\n",
    "\n",
    "for dt in alldata_df_agg_monthly.dt_business.unique():\n",
    "\n",
    "    key = str(dt).split('T')[0]\n",
    "\n",
    "    df_monthly[key] = alldata_df_agg_monthly[alldata_df_agg_monthly.dt_business==dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_generate_dict(df,financial_instrument_agg, sett_agg):\n",
    "\n",
    "    df_dict = dict()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    if not financial_instrument_agg and not sett_agg:\n",
    "\n",
    "        fi_list = df['cd_sec_at'].unique()\n",
    "\n",
    "        sett_list = df['cd_sett'].unique()\n",
    "\n",
    "        \n",
    "\n",
    "        for fi in fi_list:\n",
    "\n",
    "            for sett in sett_list:\n",
    "\n",
    "                df_dict[fi+\"_\"+sett] = df[ (df['cd_sec_at'] == fi) & (df['cd_sett'] == sett)]\n",
    "\n",
    "                \n",
    "\n",
    "    if not financial_instrument_agg and  sett_agg:\n",
    "\n",
    "        fi_list = df['cd_sec_at'].unique()\n",
    "\n",
    "        for fi in fi_list:\n",
    "\n",
    "            df_dict[fi] = df[ (df['cd_sec_at'] == fi) ]\n",
    "\n",
    "    \n",
    "\n",
    "    if financial_instrument_agg and not sett_agg:\n",
    "\n",
    "        sett_list = df['cd_sett'].unique()\n",
    "\n",
    "        for sett in sett_list:\n",
    "\n",
    "            df_dict[sett] = df[ (df['cd_sett'] == sett)]\n",
    "\n",
    "            \n",
    "\n",
    "    if financial_instrument_agg and  sett_agg:\n",
    "\n",
    "        df_dict['all'] = df\n",
    "\n",
    "        \n",
    "\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_to_graph(df):\n",
    "\n",
    "    df['am_pend']= df['am_pend'].astype(int)\n",
    "\n",
    "    df['am_amt']=  df['am_amt'].astype(int)\n",
    "\n",
    "    \n",
    "\n",
    "    df = df[(df['am_amt'] != 0) | (df['am_pend'] != 0 )]\n",
    "\n",
    "    \n",
    "\n",
    "    df = df[(df['cd_sett'] == \"S\") | (df['cd_sett']==\"N\")]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, date_range='all', financial_instrument_agg = False,sett_agg=False,  filter_on_sett= False,direction= 'D'):\n",
    "\n",
    "    \n",
    "\n",
    "    all_musk = ['dt_business','ds_deli_pty1','ds_rece_pty1','cd_sett','cd_sec_at']\n",
    "\n",
    "    if date_range == 'all':\n",
    "\n",
    "        all_musk.remove('dt_business')\n",
    "\n",
    "    if date_range == 'Y':\n",
    "\n",
    "        return\n",
    "\n",
    "    if date_range == 'M':\n",
    "\n",
    "        return\n",
    "\n",
    "    if date_range == 'W':\n",
    "\n",
    "        return \n",
    "\n",
    "    if date_range == 'D':\n",
    "\n",
    "        return\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    if financial_instrument_agg:\n",
    "\n",
    "        all_musk.remove('cd_sec_at')\n",
    "\n",
    "        \n",
    "\n",
    "    if sett_agg:\n",
    "\n",
    "        all_musk.remove('cd_sett')\n",
    "\n",
    "\n",
    "\n",
    "    df = df.groupby(all_musk)[['am_pend','am_amt']].sum().reset_index()\n",
    "\n",
    "    \n",
    "\n",
    "    df_pp = preprocess_to_graph(df)\n",
    "\n",
    "    df_pp_dict = df_generate_dict(df_pp,financial_instrument_agg, sett_agg)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "    graph_dict = dict()\n",
    "\n",
    "    for df_name in df_pp_dict:\n",
    "\n",
    "        src_list = list(df_pp_dict[df_name]['ds_deli_pty1'])\n",
    "\n",
    "        dst_list = list(df_pp_dict[df_name]['ds_rece_pty1'])\n",
    "\n",
    "\n",
    "\n",
    "        if anonymize_data:\n",
    "\n",
    "            src_list = check_anonymized(src_list)\n",
    "\n",
    "            dst_list = check_anonymized(dst_list)\n",
    "\n",
    "     \n",
    "\n",
    "        \n",
    "\n",
    "        if 'S' in df_name or 'R' in df_name:\n",
    "\n",
    "            w_list = list(df_pp_dict[df_name]['am_amt'])\n",
    "\n",
    "        else:\n",
    "\n",
    "            w_list = list(df_pp_dict[df_name]['am_pend'])\n",
    "\n",
    "\n",
    "\n",
    "        if direction == 'D':\n",
    "\n",
    "            G = nx.DiGraph()\n",
    "\n",
    "        else:\n",
    "\n",
    "            G = nx.Graph()\n",
    "\n",
    "        assert(len(src_list) == len(dst_list) == len(w_list))\n",
    "\n",
    "        for index in range(len(src_list)):\n",
    "\n",
    "\n",
    "\n",
    "            src =  src_list[index]\n",
    "\n",
    "            dst = dst_list[index]\n",
    "\n",
    "            w = w_list[index]\n",
    "\n",
    "          \n",
    "\n",
    "            if src is None or dst is None or w ==0:\n",
    "\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "\n",
    "                G.add_weighted_edges_from([(src,dst,int(w))])\n",
    "\n",
    "        assert len(G.nodes()) ==len(list(set(df_pp_dict[df_name]['ds_deli_pty1'].unique()) | set(df_pp_dict[df_name]['ds_rece_pty1'].unique()))), \"Number of nodes and company names must be equal\"\n",
    "\n",
    "        assert len(G.edges())== len(df_pp_dict[df_name]) , \"Number of edges and number of dataframe rows must be equal\"\n",
    "\n",
    "\n",
    "\n",
    "        graph_dict[df_name] = G\n",
    "\n",
    "    return graph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_dict = create_graph(alldata_df,financial_instrument_agg=False,sett_agg = False, filter_on_sett= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_dict_ETF = create_graph(alldata_df[alldata_df['ind_etf_mkt'] == 1],financial_instrument_agg=True,sett_agg = False, filter_on_sett= True)\n",
    "\n",
    "G_dict_ETF['ETF_N'] = G_dict_ETF.pop(\"N\")\n",
    "\n",
    "G_dict_ETF['ETF_S'] = G_dict_ETF.pop(\"S\")\n",
    "\n",
    "G_dict = {**G_dict_ETF, **G_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_dict_monthly= dict()\n",
    "\n",
    "df_concat = pd.DataFrame()\n",
    "\n",
    "for key in df_monthly:\n",
    "\n",
    "    df = df_monthly[key]    \n",
    "\n",
    "    G_tmp = create_graph(df,financial_instrument_agg=False,sett_agg = False, filter_on_sett= True)\n",
    "\n",
    "    G_tmp_ETF = create_graph(df[df['ind_etf_mkt'] == 1],financial_instrument_agg=True,sett_agg = False, filter_on_sett= True)\n",
    "\n",
    "    G_tmp_ETF['ETF_N'] = G_tmp_ETF.pop(\"N\")\n",
    "\n",
    "    G_tmp_ETF['ETF_S'] = G_tmp_ETF.pop(\"S\")\n",
    "\n",
    "    G_dict_monthly[key] = {**G_tmp_ETF, **G_tmp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_dict_monthly_cumulative= dict()\n",
    "\n",
    "df_concat = pd.DataFrame()\n",
    "\n",
    "for key in df_monthly:\n",
    "\n",
    "    df = df_monthly[key]\n",
    "\n",
    "    df_concat = pd.concat([df_concat,df])\n",
    "\n",
    "    \n",
    "\n",
    "    G_tmp = create_graph(df_concat,financial_instrument_agg=False,sett_agg = False, filter_on_sett= True)\n",
    "\n",
    "#    print(df_concat.shape)\n",
    "\n",
    "    G_tmp_ETF = create_graph(df_concat[df_concat['ind_etf_mkt'] == 1],financial_instrument_agg=True,sett_agg = False, filter_on_sett= True)\n",
    "\n",
    "    G_tmp_ETF['ETF_N'] = G_tmp_ETF.pop(\"N\")\n",
    "\n",
    "    G_tmp_ETF['ETF_S'] = G_tmp_ETF.pop(\"S\")\n",
    "\n",
    "    G_dict_monthly_cumulative[key] = {**G_tmp_ETF, **G_tmp}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK ORDER AND SIZE of Final DF\n",
    "\n",
    "\n",
    "\n",
    "for key in G_dict_monthly_cumulative:   \n",
    "\n",
    "    for instr in  G_dict_monthly_cumulative[key].keys():\n",
    "\n",
    "        if key == list(G_dict_monthly_cumulative.keys())[-1]:\n",
    "\n",
    "            assert G_dict_monthly_cumulative[key][instr].order() == G_dict[instr].order(), \"Order between time graph at last step is different from Graph of aggregated for all dates\"\n",
    "\n",
    "            assert G_dict_monthly_cumulative[key][instr].size() == G_dict[instr].size(), \"Size between time graph at last step is different from Graph of aggregated for all dates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_power_law_exponent(degree_list):        \n",
    "\n",
    "    fit = powerlaw.Fit(degree_list)\n",
    "\n",
    "    \n",
    "\n",
    "    alpha = fit.power_law.alpha\n",
    "\n",
    "    xmin = fit.power_law.xmin\n",
    "\n",
    "    test, p = kstest(degree_list, \"powerlaw\", args = (alpha,xmin),N=len(degree_list))\n",
    "\n",
    "    return {'p-value':p,'test':test,'exp': alpha}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tenth_centrality(G, centr_type ,w = None):\n",
    "\n",
    "    if centr_type == 'degree':\n",
    "\n",
    "        centr = nx.degree_centrality(G)\n",
    "\n",
    "    elif centr_type == 'closeness':\n",
    "\n",
    "        centr = nx.closeness_centrality(G)\n",
    "\n",
    "    elif centr_type == 'betweenness':\n",
    "\n",
    "        centr = nx.betweenness_centrality(G, weight= w)\n",
    "\n",
    "    elif centr_type == 'eigenvector':\n",
    "\n",
    "        try:\n",
    "\n",
    "            centr = nx.eigenvector_centrality(G, weight= w)\n",
    "\n",
    "        except:\n",
    "\n",
    "            centr = \"No Eigenvector Centrality\"\n",
    "\n",
    "            centr = float('nan')\n",
    "\n",
    "            return centr\n",
    "\n",
    "    elif centr_type == 'pagerank':\n",
    "\n",
    "        centr = nx.pagerank(G, weight= w)\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise Exception(\"Centrality type not found [try degree, betweenness, closeness, eigenvector, pagerank]\")\n",
    "\n",
    "    sort_orders = sorted(centr.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sort_orders[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_graph_stats(G, name=\"\",str_to_write = \"\"):\n",
    "\n",
    "    \n",
    "\n",
    "    df = dict()\n",
    "\n",
    "    str_to_write = \"\"\n",
    "\n",
    "    order = G.order()\n",
    "\n",
    "    size = G.size()\n",
    "\n",
    "    if order <= 0: \n",
    "\n",
    "        return {\"string_to_write\":\"Not applicable, size or order are equal to 0\"}\n",
    "\n",
    "    \n",
    "\n",
    "    if order <=3:\n",
    "\n",
    "        return {\"string_to_write\":\"Not applicable, size or order lesser than 3\"}\n",
    "\n",
    "    try:\n",
    "\n",
    "        order_size_ratio = size/order\n",
    "\n",
    "      \n",
    "\n",
    "    except:\n",
    "\n",
    "          order_size_ratio = \"NA\"\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    str_to_write+=f'\\nâ¸Number of nodes: {order} - Number of links:{size} - Size/Order ratio: {order_size_ratio}'\n",
    "\n",
    "    degree= list(dict(G.degree()).values())\n",
    "\n",
    "    \n",
    "\n",
    "    str_to_write+=f'\\nâ¸Standard deviation: {np.std(degree)}'\n",
    "\n",
    "    str_to_write+= f'\\nâ¸Mean: {np.mean(degree)}'\n",
    "\n",
    "    str_to_write+= f'\\nâ¸Median: {np.median(degree)}'\n",
    "\n",
    "    str_to_write+= f'\\nâ¸Min: {np.min(degree)}'\n",
    "\n",
    "    str_to_write+= f'\\nâ¸Max: {np.max(degree)}'\n",
    "\n",
    "    \n",
    "\n",
    "    in_degree = list(dict(G.in_degree()).values())\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Standard deviation in_degree: {np.std(in_degree)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Mean in_degree: {np.mean(in_degree)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Median in_degree: {np.median(in_degree)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Min in_degree: {np.min(in_degree)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Max in_degree: {np.max(in_degree)}'\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    out_degree = list(dict(G.out_degree()).values())\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Standard deviation out_degree: {np.std(out_degree)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Mean out_degree: {np.mean(out_degree)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Median out_degree: {np.median(out_degree)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Min out_degree: {np.min(out_degree)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Max out_degree: {np.max(out_degree)}'\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    degree_weighted= list(dict(G.degree(weight=\"weight\")).values())\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Standard deviation weighted: {np.std(degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Mean weighted: {np.mean(degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Median weighted: {np.median(degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Min weighted: {np.min(degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Max weighted: {np.max(degree_weighted)}'\n",
    "\n",
    "    \n",
    "\n",
    "    in_degree_weighted = list(dict(G.in_degree(weight=\"weight\")).values())\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Standard deviation in_degree weighted: {np.std(in_degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Mean in_degree weighted: {np.mean(in_degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Median in_degree weighted: {np.median(in_degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Min in_degree weighted: {np.min(in_degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Max in_degree weighted: {np.max(in_degree_weighted)}'\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    out_degree_weighted = list(dict(G.out_degree(weight=\"weight\")).values())\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Standard deviation out_degree weighted: {np.std(out_degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Mean out_degree weighted: {np.mean(out_degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Median out_degree weighted: {np.median(out_degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Min out_degree weighted: {np.min(out_degree_weighted)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Max out_degree weighted: {np.max(out_degree_weighted)}'\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    density = nx.density(G)\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Density: {density}'\n",
    "\n",
    "\n",
    "\n",
    "    avg_clustering = nx.average_clustering(G)\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Avg. Clustering coeff: {avg_clustering}'\n",
    "\n",
    "    transitivity = nx.transitivity(G)\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Transitivity: {transitivity}'\n",
    "\n",
    "    assortativity =  str(nx.degree_assortativity_coefficient(G))\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Assortativity coefficient: {assortativity}'\n",
    "\n",
    "    \n",
    "\n",
    "    assortativity_w =  str(nx.degree_assortativity_coefficient(G,weight='weigth'))\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Assortativity weighted coefficient: {assortativity_w}'\n",
    "\n",
    "    \n",
    "\n",
    "    pearson_assortativity = nx.degree_pearson_correlation_coefficient(G)\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Pearson Assortativity coefficient: {pearson_assortativity}' \n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        avg_shortest_path_length = nx.average_shortest_path_length(G)\n",
    "\n",
    "    except nx.NetworkXError:\n",
    "\n",
    "        avg_shortest_path_length = \"is weakly connected\"\n",
    "\n",
    "        \n",
    "\n",
    "    dag = nx.is_directed_acyclic_graph(G)\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        diameter = nx.algorithms.distance_measures.diameter(G)\n",
    "\n",
    "    except:\n",
    "\n",
    "        diameter = float(\"inf\")\n",
    "\n",
    "\n",
    "\n",
    "    wk_comps = [len(c) for c in sorted(nx.weakly_connected_components(G),key=len, reverse=True)]\n",
    "\n",
    "    is_weak =  nx.is_strongly_connected(G)\n",
    "\n",
    "    sg_comps = [len(c) for c in sorted(nx.strongly_connected_components(G),key=len, reverse=True)]\n",
    "\n",
    "    is_strong = nx.is_weakly_connected(G)\n",
    "\n",
    "    \n",
    "\n",
    "    str_to_write+=f'\\nâ¸Average Shortest Path Length: {avg_shortest_path_length}'\n",
    "\n",
    "    str_to_write+= f'\\nâ¸Diameter: {diameter}'\n",
    "\n",
    "    str_to_write+= f'\\nâ¸Is DAG?: {dag}'\n",
    "\n",
    "\n",
    "\n",
    "    str_to_write+= f'\\nâ¸Weakly Connected Components: {wk_comps}'\n",
    "\n",
    "    str_to_write+= f'\\nâ¸Is Weakly connected?:  {is_weak}'\n",
    "\n",
    "    \n",
    "\n",
    "    str_to_write+= f'\\nâ¸Strongly Connected Components: {sg_comps}'\n",
    "\n",
    "    str_to_write+= f'\\nâ¸Is Strongly connected?:  {is_strong}'   \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    deg_centr = nx.degree_centrality(G)\n",
    "\n",
    "    sort_orders_dc = sorted(deg_centr.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "    #for i in range(10):\n",
    "\n",
    "        #print(sort_orders_dc[i])\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    degree_Centrality = tenth_centrality(G, centr_type=\"degree\")\n",
    "\n",
    "    betweenesCentrality = tenth_centrality(G, centr_type=\"betweenness\")\n",
    "\n",
    "    closenessCentrality = tenth_centrality(G, centr_type=\"closeness\")\n",
    "\n",
    "    eigenCentrality = tenth_centrality(G, centr_type=\"eigenvector\")\n",
    "\n",
    "    pagerankCentrality = tenth_centrality(G, centr_type=\"pagerank\")\n",
    "\n",
    "    \n",
    "\n",
    "    betweenesCentrality_w = tenth_centrality(G, centr_type=\"betweenness\", w='weight')\n",
    "\n",
    "    eigenCentrality_w = tenth_centrality(G, centr_type=\"eigenvector\", w='weight')\n",
    "\n",
    "    pagerankCentrality_w = tenth_centrality(G, centr_type=\"pagerank\", w='weight')\n",
    "\n",
    "\n",
    "\n",
    "    str_to_write+=f'\\nâ¸10 most important nodes for Degree Centrality:\\n{degree_Centrality}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸10 most important nodes for Betweennes Centrality:\\n{betweenesCentrality}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸10 most important nodes for Closeness Centrality:\\n{closenessCentrality}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸10 most important nodes for Eigenvector Centrality:\\n{eigenCentrality}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸10 most important nodes for Page Rank:\\n{pagerankCentrality}'\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    str_to_write+=f'\\nâ¸10 most important nodes for Betweennes Centrality Weighted:\\n{betweenesCentrality}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸10 most important nodes for Eigenvector Centrality Weighted:\\n{eigenCentrality}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸10 most important nodes for Page Rank Weighted:\\n{pagerankCentrality}'\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    percentile_90 = np.percentile(degree,90)\n",
    "\n",
    "    str_to_write+=f'\\nâ¸90-percentile degree: {percentile_90}'\n",
    "\n",
    "    hub_nodi = [k for k,v in dict(G.degree()).items() if v>= percentile_90]\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Number of nodes in HUBs: {len(hub_nodi)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸List of nodes in HUBs:\\n{list(hub_nodi)}'\n",
    "\n",
    "\n",
    "\n",
    "    percentile_90_in = np.percentile(in_degree,90)\n",
    "\n",
    "    str_to_write+=f'\\nâ¸90-percentile degree: {percentile_90_in}'\n",
    "\n",
    "    hub_nodi_in = [k for k,v in dict(G.degree()).items() if v>= percentile_90_in]\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Number of nodes in HUBs: {len(hub_nodi_in)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸List of nodes in HUBs:\\n{list(hub_nodi_in)}'\n",
    "\n",
    "\n",
    "\n",
    "    percentile_90_out = np.percentile(out_degree,90)\n",
    "\n",
    "    str_to_write+=f'\\nâ¸90-percentile degree: {percentile_90_out}'\n",
    "\n",
    "    hub_nodi_out = [k for k,v in dict(G.degree()).items() if v>= percentile_90_out]\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Number of nodes in HUBs: {len(hub_nodi_out)}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸List of nodes in HUBs:\\n{list(hub_nodi_out)}'\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    isolates = list(nx.isolates(G))\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Isolated nodes:{isolates}'\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # Not working on directed Graphs\n",
    "\n",
    "    #print(\"Network connected?\",nx.is_connected(G))\n",
    "\n",
    "    #print(\"# Connected components\",nx.number_connected_components(G))\n",
    "\n",
    "    #triangles = len(nx.triangles(G))\n",
    "\n",
    "    #print(\"Number of triangles:\",triangles)\n",
    "\n",
    "   \n",
    "\n",
    "    deg_PL = compute_power_law_exponent(degree)\n",
    "\n",
    "    deg_W_PL = compute_power_law_exponent(degree_weighted)\n",
    "\n",
    "    deg_in_PL = compute_power_law_exponent(in_degree)\n",
    "\n",
    "    deg_in_W_PL= compute_power_law_exponent(in_degree_weighted)\n",
    "\n",
    "    deg_out_PL= compute_power_law_exponent(out_degree)\n",
    "\n",
    "    deg_out_W_PL = compute_power_law_exponent(out_degree_weighted)\n",
    "\n",
    "    \n",
    "\n",
    "  #  str_to_write+=f'\\nâ¸K-test for PowerLaw distribution'\n",
    "\n",
    "\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Power Law K-test on Degree: {deg_PL}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Power Law K-test on Weighted Degree: {deg_W_PL}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Power Law K-test on In-Degree: {deg_in_PL}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Power Law K-test on Weighted In-Degree: {deg_in_W_PL}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Power Law K-test on Out-Degree: {deg_out_PL}'\n",
    "\n",
    "    str_to_write+=f'\\nâ¸Power Law K-test on Weighted Out-Degree: {deg_out_W_PL}'\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return {'string_to_write':str_to_write,'order':order,'size':size, 'order_size_ratio': order_size_ratio, 'avg_shortest_path_length': avg_shortest_path_length,\\\n",
    "\n",
    "            'mean':np.mean(degree),'std':np.std(degree),'median':np.median(degree),'min_deg':np.min(degree),'max_deg':np.max(degree),\\\n",
    "\n",
    "            \\\n",
    "\n",
    "            'mean_in':np.mean(in_degree),'std_in':np.std(in_degree),\\\n",
    "\n",
    "            'median_in':np.median(in_degree),'min_deg_in':np.min(in_degree),'max_deg_in':np.max(in_degree),\\\n",
    "\n",
    "            \\\n",
    "\n",
    "            'mean_out':np.mean(out_degree),'std_out':np.std(out_degree),\\\n",
    "\n",
    "            'median_out':np.median(out_degree),'min_deg_out':np.min(out_degree),'max_deg_out':np.max(out_degree),\\\n",
    "\n",
    "            \\\n",
    "\n",
    "            'mean_weighted':np.mean(degree_weighted),'std_weighted':np.std(degree_weighted),\\\n",
    "\n",
    "            'median_weighted':np.median(degree_weighted),'min_deg_weighted':np.min(degree_weighted),'max_deg_weighted':np.max(degree_weighted),\\\n",
    "\n",
    "            \\\n",
    "\n",
    "            'mean_in_weighted':np.mean(in_degree_weighted),'std_in_weighted':np.std(in_degree_weighted),\\\n",
    "\n",
    "            'median_in_weighted':np.median(in_degree_weighted),'min_deg_in_weighted':np.min(in_degree_weighted),'max_deg_in_weighted':np.max(in_degree_weighted),\\\n",
    "\n",
    "            \\\n",
    "\n",
    "            'mean_out_weighted':np.mean(out_degree_weighted),'std_out_weighted':np.std(out_degree_weighted),\\\n",
    "\n",
    "            'median_out_weighted':np.median(out_degree_weighted),'min_deg_out_weighted':np.min(out_degree_weighted),'max_deg_out_weighted':np.max(out_degree_weighted),\\\n",
    "\n",
    "            \\\n",
    "\n",
    "            'assortativity':assortativity, 'pearson_assortativity':pearson_assortativity, 'assortativity_weigthed':assortativity_w, \\\n",
    "\n",
    "            'transitivity':transitivity,'avg_clustering':avg_clustering,'density':density,\\\n",
    "\n",
    "            'diameter':diameter, 'is_dag':dag, 'wk_comps':wk_comps, 'is_weak':is_weak, 'sg_comps':sg_comps, 'is_strong':is_strong,\\\n",
    "\n",
    "            'degree_centrality':degree_Centrality,'betweennes_centrality':betweenesCentrality, 'closeness_centrality':closenessCentrality, \\\n",
    "\n",
    "            'eigen_centrality':eigenCentrality,'pagerank_centrality':pagerankCentrality, \\\n",
    "\n",
    "            'betweennes_centrality_weighted':betweenesCentrality_w,'eigen_centrality_weighted':eigenCentrality_w,\\\n",
    "\n",
    "            'pagerank_centrality_weighted':pagerankCentrality_w,\\\n",
    "\n",
    "            '90-percentile_degree':percentile_90,'hubs':list(hub_nodi),\\\n",
    "\n",
    "            'hubs_number':len(hub_nodi),'isolated_nodes':isolates, \\\n",
    "\n",
    "            'PL_degree_p':deg_PL['p-value'], 'PL_degree_t':deg_PL['test'],'PL_degree_exp':deg_PL['exp'], \\\n",
    "\n",
    "            'PL_degree_weighted_p': deg_W_PL['p-value'],'PL_degree_weighted_t': deg_W_PL['test'], 'PL_degree_weighted_exp': deg_W_PL['exp'],  \\\n",
    "\n",
    "            'PL_in_degree_p': deg_in_PL['p-value'], 'PL_in_degree_t': deg_in_PL['test'], 'PL_in_degree_exp': deg_in_PL['exp'],\\\n",
    "\n",
    "            'PL_in_degree_weighted_p': deg_in_W_PL['p-value'],'PL_in_degree_weighted_t': deg_in_W_PL['test'],'PL_in_degree_weighted_exp': deg_in_W_PL['exp'],\\\n",
    "\n",
    "            'PL_out_degree_p': deg_out_PL['p-value'],  'PL_out_degree_t': deg_out_PL['test'], 'PL_out_degree_exp': deg_out_PL['exp'],\\\n",
    "\n",
    "            'PL_out_degree_weighted_p': deg_out_W_PL['p-value'], 'PL_out_degree_weighted_t': deg_out_W_PL['test'],'PL_out_degree_weighted_exp': deg_out_W_PL['exp'] \\\n",
    "\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = dict() \n",
    "\n",
    "os.makedirs(ROOT_PATH+\"stats/\",exist_ok=True)\n",
    "\n",
    "file = open(ROOT_PATH+\"stats/aggregated_stats.txt\", \"w\")\n",
    "\n",
    "str_to_write_merged = \"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for g_name in G_dict:\n",
    "\n",
    "    str_to_write_merged += \"\\n\\n============ \"+g_name+\" ============\\n\"\n",
    "\n",
    "    df_stats[g_name] = compute_graph_stats(G_dict[g_name], name=g_name, str_to_write=\"\")\n",
    "\n",
    "    str_to_write_merged+=  df_stats[g_name]['string_to_write']\n",
    "\n",
    "    str_to_write_merged+=\"\\n\"\n",
    "\n",
    "\n",
    "\n",
    "file.write(str_to_write_merged)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_month_dict = dict() \n",
    "\n",
    "os.makedirs(ROOT_PATH+\"stats/\",exist_ok=True)\n",
    "\n",
    "file = open(ROOT_PATH+\"stats/stats_monthly.txt\", \"w\")\n",
    "\n",
    "str_to_write_merged = \"\"\n",
    "\n",
    "for month in G_dict_monthly:\n",
    "\n",
    "    for g_name in G_dict_monthly[month]:\n",
    "\n",
    "        str_to_write_merged += \"\\n\\n============ \"+g_name+ \"  -  \"+month+\" ============\\n\"\n",
    "\n",
    "        stats_month_dict[g_name+\"_\"+month] = compute_graph_stats(G_dict_monthly[month][g_name], name=month+\"_\"+g_name, str_to_write=\"\")\n",
    "\n",
    "        str_to_write_merged+=  stats_month_dict[g_name+\"_\"+month]['string_to_write']\n",
    "\n",
    "        str_to_write_merged+=\"\\n\"\n",
    "\n",
    "\n",
    "\n",
    "file.write(str_to_write_merged)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_month_dict_cumulative = dict() \n",
    "\n",
    "os.makedirs(ROOT_PATH+\"stats/\",exist_ok=True)\n",
    "\n",
    "file = open(ROOT_PATH+\"stats_monthly_cumulative.txt\", \"w\")\n",
    "\n",
    "\n",
    "\n",
    "str_to_write_merged = \"\"\n",
    "\n",
    "for month in G_dict_monthly_cumulative:\n",
    "\n",
    "    for g_name in G_dict_monthly_cumulative[month]:\n",
    "\n",
    "        str_to_write_merged += \"\\n\\n============ \"+g_name+ \"  -  \"+month+\" ============\\n\"\n",
    "\n",
    "        stats_month_dict_cumulative[g_name+\"_\"+month] = compute_graph_stats(G_dict_monthly_cumulative[month][g_name], name=month+\"_\"+g_name, str_to_write=\"\")\n",
    "\n",
    "        str_to_write_merged+=  stats_month_dict_cumulative[g_name+\"_\"+month]['string_to_write']\n",
    "\n",
    "        str_to_write_merged+=\"\\n\"\n",
    "\n",
    "\n",
    "\n",
    "file.write(str_to_write_merged)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame.from_dict(df_stats, orient='index')\n",
    "\n",
    "stats_df=stats_df.drop('string_to_write',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_month = pd.DataFrame.from_dict(stats_month_dict, orient='index')\n",
    "\n",
    "df_stats_month=df_stats_month.drop('string_to_write',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_month.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_month_cumulative = pd.DataFrame.from_dict(stats_month_dict_cumulative, orient='index')\n",
    "\n",
    "df_stats_month_cumulative=df_stats_month_cumulative.drop('string_to_write',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_month_cumulative.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Centrality Analysis**\n",
    "\n",
    "The idea is to detect the most central nodes in the network each month. Different techniques have been applied for the computation of the most Central nodes: Degree Centrality, Betweenness Centrality, Closeness Centrality, Eigenvector Centrality, PageRank Centrality. Using these techniques, for each month a nodes ranking is obtained. In particular, the objective is the identification of the most 10th central nodes and also their position changes over time. If a node does not change position or is not excluded from the ranking, it means that Preferential Attachment property is confirmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_algos = ['degree_centrality','betweennes_centrality','eigen_centrality',\n",
    "\n",
    "   'pagerank_centrality','betweennes_centrality_weighted','eigen_centrality_weighted','pagerank_centrality_weighted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hub_month_dict(type_dict = 'cumulative', algo='pagerank_centrality_weighted' ):\n",
    "\n",
    "    if type_dict == 'cumulative':\n",
    "\n",
    "        df_stats = df_stats_month_cumulative\n",
    "\n",
    "    if type_dict == 'month':\n",
    "\n",
    "        df_stats = df_stats_month\n",
    "\n",
    "    empty = []     \n",
    "\n",
    "    hubs_month_dict = dict()\n",
    "\n",
    "    for instr in list(set([i.split('_')[0]+\"_\"+i.split('_')[1] for i in df_stats_month_cumulative.index])):\n",
    "\n",
    "  \n",
    "\n",
    "        for index, row in df_stats[df_stats.index.str.contains(instr)][['hubs','degree_centrality','betweennes_centrality','eigen_centrality',\n",
    "\n",
    "    'pagerank_centrality','betweennes_centrality_weighted','eigen_centrality_weighted','pagerank_centrality_weighted']].iterrows():\n",
    "\n",
    "           # print(instr)\n",
    "\n",
    "            \n",
    "\n",
    "            hubs_month_dict[instr] = dict()\n",
    "\n",
    "            for index, row in df_stats[df_stats.index.str.contains(instr)][[algo]].iterrows():\n",
    "\n",
    "                split_index = index.split(\"_\")\n",
    "\n",
    "                date = split_index[2][:-3]\n",
    "\n",
    "                instr = split_index[0]+\"_\"+split_index[1]\n",
    "\n",
    "                hubs_month_dict[instr][date] = dict()\n",
    "\n",
    "                try: \n",
    "\n",
    "                    if type(row[algo]) is not list and math.isnan(row[algo]):\n",
    "\n",
    "                        empty.append(instr+\"_\"+date)\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        for el in row[algo]:\n",
    "\n",
    "                            hubs_month_dict[instr][date][el[0]] =   int(row[algo].index(el))+1                    \n",
    "\n",
    "                except TypeError:\n",
    "\n",
    "                    print(\"\\n\\nTYPEERROR\")\n",
    "\n",
    "                    print(row[algo])\n",
    "\n",
    "                    print(algo)\n",
    "\n",
    "           \n",
    "\n",
    "              \n",
    "\n",
    "    print(\"EMPTY RANKING for:\")\n",
    "\n",
    "    print(empty)\n",
    "\n",
    "    return hubs_month_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution_hubs_plot(dictionary):\n",
    "\n",
    "    dict_comp = dict()\n",
    "\n",
    "    for instr in dictionary:\n",
    "\n",
    "        for date in dictionary[instr]:\n",
    "\n",
    "            for comp in dictionary[instr][date]:\n",
    "\n",
    "                if comp in dict_comp.keys():\n",
    "\n",
    "                    dict_comp[comp]+=1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    dict_comp[comp] = 1\n",
    "\n",
    "    dict_comp = {k: v for k, v in sorted(dict_comp.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    fig = plt.figure(figsize=(15,12))\n",
    "\n",
    "    sns.set(style=\"white\", font_scale=1.5)\n",
    "\n",
    "    ax =sns.barplot(x =  list(dict_comp.keys()), y = list(dict_comp.values()))\n",
    "\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "    ax.set_title(\"Most common Central nodes\")\n",
    "\n",
    "    return ax #dict_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_month_dict_cumulative = get_hub_month_dict(type_dict = 'cumulative')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_single_plot(get_distribution_hubs_plot(hub_month_dict_cumulative), path=\"centrality_frequency/\", name =\"centrality_freq_cumulative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_month_dict = get_hub_month_dict(type_dict = 'month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = get_distribution_hubs_plot(hub_month_dict)\n",
    "\n",
    "save_single_plot(p, path=\"centrality_frequency/\", name =\"centrality_freq_monthly\")\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_node_ranking_plot(df, save =False, name = \"\", type_df ='cumulative'):\n",
    "\n",
    "    n_top_ranked = 10\n",
    "\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 10), subplot_kw=dict(ylim=(0.5, 0.5 + n_top_ranked)))\n",
    "\n",
    "\n",
    "\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(1))\n",
    "\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(1))\n",
    "\n",
    "\n",
    "\n",
    "    cmap = cm.get_cmap('tab20c', len(df.columns))\n",
    "\n",
    "    cNorm  = colors.Normalize(vmin=0, vmax=len(df.columns))\n",
    "\n",
    "    scalarMap = cm.ScalarMappable(norm=cNorm, cmap=cmap)\n",
    "\n",
    " \n",
    "\n",
    "    for col in range(len(df.columns)):\n",
    "\n",
    "         \n",
    "\n",
    "        colorVal = scalarMap.to_rgba(col)\n",
    "\n",
    "        ax.plot(df.index,df[df.columns[col]] , \"o-\", mfc=\"w\", label = df.columns[col], color=colorVal)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    ax.grid(axis=\"x\")\n",
    "\n",
    "    plt.xlabel('Month', fontsize=18)\n",
    "\n",
    "    plt.ylabel('Rank', fontsize=16)\n",
    "\n",
    "    plt.title(name+\" - Central Nodes ranking over time\", fontsize=20)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "#    plt.tight_layout()\n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n",
    "\n",
    "              ncol=1, fancybox=True, shadow=True )\n",
    "\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "    \n",
    "\n",
    "    if save:\n",
    "\n",
    "        if 'centrality_ranking' not in os.listdir(ROOT_PATH):\n",
    "\n",
    "            os.mkdir(ROOT_PATH+'centrality_ranking')\n",
    "\n",
    "        if type_df not in os.listdir(ROOT_PATH+'centrality_ranking'):\n",
    "\n",
    "            os.mkdir(ROOT_PATH+'centrality_ranking/'+type_df)\n",
    "\n",
    "        fig.savefig(f'{ROOT_PATH}/centrality_ranking/{type_df}/{name}_ranking.png')\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    return ax\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm in centrality_algos:\n",
    "\n",
    "    type_dict = 'cumulative'\n",
    "\n",
    "    hub_month_dict_cumulative = get_hub_month_dict(type_dict = type_dict, algo= algorithm)\n",
    "\n",
    "    print(algorithm)\n",
    "\n",
    "    path = ROOT_PATH+'centrality_ranking/'+type_dict+'/'\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "    #os.mkdir(ROOT_PATH+'centrality_ranking/'+type_df)  \n",
    "\n",
    "    plots = []\n",
    "\n",
    "                \n",
    "\n",
    "    for instr in hub_month_dict_cumulative:\n",
    "\n",
    "        df = pd.DataFrame.from_dict(hub_month_dict_cumulative[instr], orient='index')\n",
    "\n",
    "        plots.append(central_node_ranking_plot(df, save =False, name = algorithm+\"_\"+instr, type_df =type_dict))\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "\n",
    "    with PdfPages(f'{path}/{type_dict}_{algorithm}.pdf') as pdf:\n",
    "\n",
    "            for p in plots:      \n",
    "\n",
    "                fig=p.get_figure()\n",
    "\n",
    "                pdf.savefig(fig,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm in centrality_algos:\n",
    "\n",
    "    type_dict = 'month'\n",
    "\n",
    "    hub_month_dict_cumulative = get_hub_month_dict(type_dict = type_dict, algo= algorithm)\n",
    "\n",
    "    print(algorithm)\n",
    "\n",
    "    path = ROOT_PATH+'centrality_ranking/'+type_dict+'/'\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "    #os.mkdir(ROOT_PATH+'centrality_ranking/'+type_df)  \n",
    "\n",
    "    plots = []\n",
    "\n",
    "                \n",
    "\n",
    "    for instr in hub_month_dict_cumulative:\n",
    "\n",
    "        df = pd.DataFrame.from_dict(hub_month_dict_cumulative[instr], orient='index')\n",
    "\n",
    "        plots.append(central_node_ranking_plot(df, save =False, name = algorithm+\"_\"+instr, type_df =type_dict))\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "\n",
    "    with PdfPages(f'{path}/{type_dict}cumulative_{algorithm}.pdf') as pdf:\n",
    "\n",
    "            for p in plots:      \n",
    "\n",
    "                fig=p.get_figure()\n",
    "\n",
    "                pdf.savefig(fig,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in df_stats_month[df_stats_month.index.str.contains('ETF_N')][['hubs',\n",
    "\n",
    "'degree_centrality',\n",
    "\n",
    "'betweennes_centrality',\n",
    "\n",
    "'eigen_centrality',\n",
    "\n",
    "'closeness_centrality',\n",
    "\n",
    "'pagerank_centrality',\n",
    "\n",
    "'betweennes_centrality_weighted',\n",
    "\n",
    "'eigen_centrality_weighted',\n",
    "\n",
    "'pagerank_centrality_weighted']]:\n",
    "\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hubname(lis):\n",
    "\n",
    "    if type(lis) is not list and math.isnan(lis):\n",
    "\n",
    "        return \"Centrality measure is not computable\"\n",
    "\n",
    "    return [i[0] for i in lis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_central_nodes(df, name=\"\"):\n",
    "\n",
    "    path = ROOT_PATH+'hubs/'+name+'/'\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    for instr in G_dict.keys():\n",
    "\n",
    "        str_file = \"\"\n",
    "\n",
    "        str_file_score = \"\\n\\n######### SCORES ###########\"\n",
    "\n",
    "        file = open(path+instr+\".txt\", \"w\")\n",
    "\n",
    "        for index, row in df[df.index.str.contains(instr)][['hubs',\n",
    "\n",
    "    'degree_centrality',\n",
    "\n",
    "    'betweennes_centrality',\n",
    "\n",
    "    'eigen_centrality',\n",
    "\n",
    "    'closeness_centrality',\n",
    "\n",
    "    'pagerank_centrality',\n",
    "\n",
    "    'betweennes_centrality_weighted',\n",
    "\n",
    "    'eigen_centrality_weighted',\n",
    "\n",
    "    'pagerank_centrality_weighted']].iterrows():\n",
    "\n",
    "         \n",
    "\n",
    "            str_file+=f\"================ {str(index)} ================\\n\\n\"\n",
    "\n",
    "            str_file+=f\"HUBS with degree in 90-percentile: {row['hubs']}\\n\\n\"\n",
    "\n",
    "            str_file+=f\"Degree Centrality: {get_hubname(row['degree_centrality'])}\\n\\n\"\n",
    "\n",
    "            str_file+=f\"Betweennes Centrality:  {get_hubname(row['betweennes_centrality'])}\\n\\n\"\n",
    "\n",
    "            str_file+=f\"Eigenvector Centrality:  {get_hubname(row['eigen_centrality'])}\\n\\n\"\n",
    "\n",
    "            str_file+=f\"Pagerank Centrality:  {get_hubname(row['pagerank_centrality'])}\\n\\n\"\n",
    "\n",
    "            str_file+=f\"Weighted Betweennes Centrality:  {get_hubname(row['betweennes_centrality_weighted'])}\\n\\n\"\n",
    "\n",
    "            str_file+=f\"Weighted Eigenvector Centrality:  {get_hubname(row['eigen_centrality_weighted'])}\\n\\n\"\n",
    "\n",
    "            str_file+=f\"Weighted Pagerank Centrality:  {get_hubname(row['pagerank_centrality_weighted'])}\\n\\n\"\n",
    "\n",
    "\n",
    "\n",
    "            str_file+=\"\\n\\n\"\n",
    "\n",
    "\n",
    "\n",
    "            str_file_score+= f\"================ Centrality SCORES: {str(index)} ================\\n\\n\"\n",
    "\n",
    "            str_file_score+=f\"Degree Centrality: {row['degree_centrality']}\\n\\n\"\n",
    "\n",
    "            str_file_score+=f\"Betweennes Centrality:  {row['betweennes_centrality']}\\n\\n\"\n",
    "\n",
    "            str_file_score+=f\"Eigenvector Centrality:  {row['eigen_centrality']}\\n\\n\"\n",
    "\n",
    "            str_file_score+=f\"Pagerank Centrality:  {row['pagerank_centrality']}\\n\\n\"\n",
    "\n",
    "            str_file_score+=f\"Weighted Betweennes Centrality:  {row['betweennes_centrality_weighted']}\\n\\n\"\n",
    "\n",
    "            str_file_score+=f\"Weighted Eigenvector Centrality:  {row['eigen_centrality_weighted']}\\n\\n\"\n",
    "\n",
    "            str_file_score+=f\"Weighted Pagerank Centrality:  {row['pagerank_centrality_weighted']}\\n\\n\"\n",
    "\n",
    "  \n",
    "\n",
    "        file.write(str_file+'\\n\\n\\n'+str_file_score)\n",
    "\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_central_nodes(df_stats_month  , name='monthly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_central_nodes(df_stats_month_cumulative  , name='cumulative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Distribution plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distribution_plots(G,g_name, graph_type):\n",
    "\n",
    "    \n",
    "\n",
    "    #pp = PdfPages(f'{ROOT_PATH}graphplots/{graph_type}/{g_name}_plots.pdf')\n",
    "\n",
    "    plots = []\n",
    "\n",
    "    \n",
    "\n",
    "    degree= list(dict(G.degree).values())\n",
    "\n",
    "    in_degree = list(dict(G.in_degree).values())\n",
    "\n",
    "    out_degree = list(dict(G.out_degree).values())\n",
    "\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(12,8))    \n",
    "\n",
    "    plt.title(\"Degree Distribution\")\n",
    "\n",
    "    txt = 'Degree distribution visualisations: '+g_name+\"_\"+graph_type\n",
    "\n",
    "    plt.text(0.5,0.95,txt, transform=fig.transFigure, size=24, ha='center')\n",
    "\n",
    "    plt.plot(sorted(degree,reverse=True))\n",
    "\n",
    "    \n",
    "\n",
    "    plots.append(fig)\n",
    "\n",
    "    #pp.savefig()\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    degree_sequence = sorted([d for n, d in G.degree()], reverse=True)  # degree sequence\n",
    "\n",
    "    degreeCount = collections.Counter(degree_sequence)\n",
    "\n",
    "    deg, cnt = zip(*degreeCount.items())\n",
    "\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(12,8))    \n",
    "\n",
    "    plt.bar(deg, cnt, width=0.8, color=\"b\")\n",
    "\n",
    "    plt.title(\"Degree Histogram\")\n",
    "\n",
    "    plt.ylabel(\"Nodes\")\n",
    "\n",
    "    plt.xlabel(\"Degree\")\n",
    "\n",
    "    plots.append(fig)\n",
    "\n",
    "   # pp.savefig()\n",
    "\n",
    "   # plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(12,8))    \n",
    "\n",
    "    plt.bar(x =cnt, height= deg)\n",
    "\n",
    "    plt.axvline(np.array(deg).mean(), color='k', linestyle='dashed', linewidth=2)\n",
    "\n",
    "    plt.title(\"Degree Histogram \"+g_name+\"_\"+graph_type)\n",
    "\n",
    "    plt.xlabel('Number of connections per node')\n",
    "\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plots.append(fig)\n",
    "\n",
    "    #pp.savefig()\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(12,8)) \n",
    "\n",
    "    plt.title(\"Degree Distribution \"+g_name+\"_\"+graph_type)\n",
    "\n",
    "    plt.plot(deg,cnt,\"ro-\") # degree\n",
    "\n",
    "    plt.legend(['Degree'])\n",
    "\n",
    "    plt.xlabel('Degree')\n",
    "\n",
    "    plt.ylabel('Number of nodes')\n",
    "\n",
    "    plt.title('Network')\n",
    "\n",
    "    #pp.savefig()\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "    plots.append(fig)\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(12,8))    \n",
    "\n",
    "    plt.loglog(deg,cnt,\"ro\") # degree\n",
    "\n",
    "    plt.legend(['Degree'])\n",
    "\n",
    "    plt.xlabel('Degree')\n",
    "\n",
    "    plt.ylabel('Number of nodes')\n",
    "\n",
    "    plt.title('Log Network '+g_name+\"_\"+graph_type)\n",
    "\n",
    "    #pp.savefig()\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "    plots.append(fig)\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # ECDF linear scale\n",
    "\n",
    "    cdf = ECDF(degree)\n",
    "\n",
    "    x = np.unique(degree)\n",
    "\n",
    "    y = cdf(x)\n",
    "\n",
    "\n",
    "\n",
    "    fig_cdf = plt.figure(figsize=(12,8))\n",
    "\n",
    "    axes = fig_cdf.gca()\n",
    "\n",
    "    plt.title(\"ECDF Linear Scale \"+g_name+\"_\"+graph_type)\n",
    "\n",
    "\n",
    "\n",
    "    axes.plot(x,y,marker='o',ms=6, linestyle='None')\n",
    "\n",
    "    axes.set_xlabel('Degree',size=20)\n",
    "\n",
    "    axes.set_ylabel('ECDF', size = 20)\n",
    "\n",
    "    plots.append(fig_cdf)\n",
    "\n",
    "    #pp.savefig()\n",
    "\n",
    "    #plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # ECDF loglog scale\n",
    "\n",
    "    cdf = ECDF(degree)\n",
    "\n",
    "    x = np.unique(degree)\n",
    "\n",
    "    y = cdf(x)\n",
    "\n",
    "    fig_cdf = plt.figure(figsize=(12,8))\n",
    "\n",
    "    axes = fig_cdf.gca()\n",
    "\n",
    "    plt.title(\"ECDF Log-Log Scale \"+g_name+\"_\"+graph_type)\n",
    "\n",
    "\n",
    "\n",
    "    axes.loglog(x,y,marker='o',ms=8, linestyle='--')\n",
    "\n",
    "    axes.set_xlabel('Degree',size=20)\n",
    "\n",
    "    axes.set_ylabel('ECDF', size = 20)\n",
    "\n",
    "    plots.append(fig_cdf)\n",
    "\n",
    "    #pp.savefig()\n",
    "\n",
    "    #plt.close()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # ECCDF\n",
    "\n",
    "    cdf = ECDF(degree)\n",
    "\n",
    "    x = np.unique(degree)\n",
    "\n",
    "    y = cdf(x)\n",
    "\n",
    "    fig_cdf = plt.figure(figsize=(12,8))\n",
    "\n",
    "    plt.title(\"ECCDF \"+g_name+\"_\"+graph_type)\n",
    "\n",
    "\n",
    "\n",
    "    axes = fig_cdf.gca()\n",
    "\n",
    "    axes.loglog(x,1-y,marker='o',ms=8, linestyle='--')\n",
    "\n",
    "    axes.set_xlabel('Degree',size=20)\n",
    "\n",
    "    axes.set_ylabel('ECCDF', size = 20)\n",
    "\n",
    "    plots.append(fig_cdf)\n",
    "\n",
    "    #pp.savefig()\n",
    "\n",
    "    #plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    SF = nx.generators.directed.scale_free_graph(len(G.nodes))\n",
    "\n",
    "    sf_deg = list(dict(SF.degree()))\n",
    "\n",
    "    \n",
    "\n",
    "    # ECCDF\n",
    "\n",
    "    cdf = ECDF(sf_deg)\n",
    "\n",
    "    x = np.unique(sf_deg)\n",
    "\n",
    "    y = cdf(x)\n",
    "\n",
    "    fig_cdf = plt.figure(figsize=(12,8))\n",
    "\n",
    "    axes = fig_cdf.gca()\n",
    "\n",
    "    axes.loglog(x,1-y,marker='o',ms=8, linestyle='--')\n",
    "\n",
    "    axes.set_xlabel('Degree',size=20)\n",
    "\n",
    "    axes.set_ylabel('ECCDF', size = 20)\n",
    "\n",
    "    plt.title('ECCDF Scale Free Network '+g_name+\"_\"+graph_type)\n",
    "\n",
    "    plots.append(fig_cdf)\n",
    "\n",
    "    #pp.savefig()\n",
    "\n",
    "    #plt.close()\n",
    "\n",
    "    \n",
    "\n",
    "    density = p = nx.density(G)\n",
    "\n",
    "    p = density\n",
    "\n",
    "    random_graph = nx.fast_gnp_random_graph(G.order(),p, directed=True)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    random_degree = list(dict(random_graph.degree()).values())\n",
    "\n",
    "    \n",
    "\n",
    "    cdf_random = ECDF(random_degree)\n",
    "\n",
    "    x_random = np.unique(random_degree)\n",
    "\n",
    "    y_random = cdf_random(x_random)\n",
    "\n",
    "\n",
    "\n",
    "    cdf = ECDF(degree)\n",
    "\n",
    "    x = np.unique(degree)\n",
    "\n",
    "    y = cdf(x)\n",
    "\n",
    "    \n",
    "\n",
    "    cdf_in = ECDF(in_degree)\n",
    "\n",
    "    x_in = np.unique(in_degree)\n",
    "\n",
    "    y_in = cdf_in(x_in)\n",
    "\n",
    "        \n",
    "\n",
    "    cdf_out = ECDF(out_degree)\n",
    "\n",
    "    x_out = np.unique(out_degree)\n",
    "\n",
    "    y_out = cdf_out(x_out)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    cdf_sf = ECDF(sf_deg)\n",
    "\n",
    "    x_sf = np.unique(sf_deg)\n",
    "\n",
    "    y_sf = cdf_sf(x_sf)\n",
    "\n",
    "    \n",
    "\n",
    "    fig_cdf_fb = plt.figure(figsize=(12,8))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    axes = fig_cdf_fb.gca()\n",
    "\n",
    "    axes.set_xscale('log')\n",
    "\n",
    "    axes.set_yscale('log')\n",
    "\n",
    "    axes.loglog(x,1-y,marker='o',label='degree',ms=8, linestyle='--')\n",
    "\n",
    "    axes.loglog(x_random,1-y_random,marker='+',label='random_net_degree',ms=10, linestyle='--')\n",
    "\n",
    "    axes.loglog(x_sf,1-y_sf,marker='+',color='r',label='scale_free_degree',ms=10, linestyle='--')\n",
    "\n",
    "    plt.title(\"ECCDF comparison Network vs. Random Network vs. Scale Free Network \"+g_name+\"_\"+graph_type)\n",
    "\n",
    "    axes.set_xlabel('Degree',size=20)\n",
    "\n",
    "    axes.set_ylabel('ECCDF', size = 20)\n",
    "\n",
    "    handles, labels = axes.get_legend_handles_labels()\n",
    "\n",
    "    axes.legend(handles, labels)\n",
    "\n",
    "    #pp.savefig()\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "    plots.append(fig_cdf_fb)\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "     \n",
    "\n",
    "    fig_cdf_fb = plt.figure(figsize=(12,8))\n",
    "\n",
    "\n",
    "\n",
    "    axes = fig_cdf_fb.gca()\n",
    "\n",
    "    axes.set_xscale('log')\n",
    "\n",
    "    axes.set_yscale('log')\n",
    "\n",
    "    axes.loglog(x,1-y,marker='o',label='degree',ms=8, linestyle='--')\n",
    "\n",
    "    axes.loglog(x_in,1-y_in,marker='o',label='in_degree',color='blue',ms=8, linestyle='--')\n",
    "\n",
    "    axes.loglog(x_out,1-y_out,marker='o',label='out_degree',color='g',ms=8, linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "    axes.loglog(x_random,1-y_random,marker='+',label='random_net_degree',ms=10, linestyle='--')\n",
    "\n",
    "    axes.loglog(x_sf,1-y_sf,marker='+',color='r',label='scale_free_degree',ms=10, linestyle='--')\n",
    "\n",
    "    plt.title(\"ECCDF comparison Network vs. Random Network vs. Scale Free Network \"+g_name+\"_\"+graph_type)\n",
    "\n",
    "    axes.set_xlabel('Degree',size=20)\n",
    "\n",
    "    axes.set_ylabel('ECCDF', size = 20)\n",
    "\n",
    "    handles, labels = axes.get_legend_handles_labels()\n",
    "\n",
    "    axes.legend(handles, labels)\n",
    "\n",
    "    #pp.savefig()\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "    plots.append(fig_cdf_fb)\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # WEIGHTED\n",
    "\n",
    "    degree_weighted= list(dict(G.degree(weight=\"weight\")).values())\n",
    "\n",
    "    in_degree_weighted = list(dict(G.in_degree(weight=\"weight\")).values())\n",
    "\n",
    "    out_degree_weighted = list(dict(G.out_degree(weight=\"weight\")).values())\n",
    "\n",
    "\n",
    "\n",
    "    cdf_deg_W = ECDF(degree_weighted)\n",
    "\n",
    "    x_deg_W = np.unique(degree_weighted)\n",
    "\n",
    "    y_deg_W = cdf_deg_W(x_deg_W)\n",
    "\n",
    "    \n",
    "\n",
    "    cdf_deg_in_W = ECDF(in_degree_weighted)\n",
    "\n",
    "    x_deg_in_W = np.unique(in_degree_weighted)\n",
    "\n",
    "    y_deg_in_W = cdf_deg_in_W(x_deg_in_W)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    cdf_deg_out_W = ECDF(out_degree_weighted)\n",
    "\n",
    "    x_deg_out_W = np.unique(out_degree_weighted)\n",
    "\n",
    "    y_deg_out_W = cdf_deg_out_W(x_deg_out_W)\n",
    "\n",
    "    \n",
    "\n",
    "    fig_cdf_fb = plt.figure(figsize=(12,8))\n",
    "\n",
    "\n",
    "\n",
    "    axes = fig_cdf_fb.gca()\n",
    "\n",
    "    axes.set_xscale('log')\n",
    "\n",
    "    axes.set_yscale('log')\n",
    "\n",
    "    axes.loglog(x_deg_W,1-y_deg_W,marker='o',label='degree_weighted',ms=8, linestyle='--')\n",
    "\n",
    "    axes.loglog(x_deg_in_W,1-y_deg_in_W,marker='o',label='in_degree_weighted',color='blue',ms=8, linestyle='--')\n",
    "\n",
    "    axes.loglog(x_deg_out_W,1-y_deg_out_W,marker='o',label='out_degree_weighted',color='g',ms=8, linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "    axes.loglog(x_random,1-y_random,marker='+',label='random_net_degree',ms=10, linestyle='--')\n",
    "\n",
    "    axes.loglog(x_sf,1-y_sf,marker='+',color='r',label='scale_free_degree',ms=10, linestyle='--')\n",
    "\n",
    "    plt.title(\"ECCDF comparison Network vs. Random Network vs. Scale Free Network - WEIGHTED Degrees \"+g_name+\"_\"+graph_type)\n",
    "\n",
    "    axes.set_xlabel('Degree',size=20)\n",
    "\n",
    "    axes.set_ylabel('ECCDF', size = 20)\n",
    "\n",
    "    handles, labels = axes.get_legend_handles_labels()\n",
    "\n",
    "    axes.legend(handles, labels)\n",
    "\n",
    "    plots.append(fig_cdf_fb)\n",
    "\n",
    "#    pp.savefig()\n",
    "\n",
    "#    plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    #plt.close()\n",
    "\n",
    "    fig = plt.figure(figsize=(12,8)) \n",
    "\n",
    "    fig.clf()\n",
    "\n",
    "\n",
    "\n",
    "    plt.title(\"Infos \"+g_name+\"_\"+graph_type)\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "    txt = 'Number of nodes: {}'.format(random_graph.order())\n",
    "\n",
    "    txt+='\\n'+'Number of links: {}'.format(random_graph.size())\n",
    "\n",
    "    txt+='\\n'+'Random Net Standard deviation: {}'.format(np.std(random_degree))\n",
    "\n",
    "    txt+='\\n'+'Random Net Mean: {}'.format(np.mean(random_degree))\n",
    "\n",
    "    txt+='\\n'+'Random Net Median: {}'.format(np.median(random_degree))\n",
    "\n",
    "    txt+='\\n'+'Random Net Min: {}'.format(np.min(random_degree))\n",
    "\n",
    "    txt+='\\n'+'Random Net Max: {}'.format(np.max(random_degree))\n",
    "\n",
    "    plt.text(0.2,0.7,txt, size=12, transform=fig.transFigure,va='center',ha='left')\n",
    "\n",
    "    \n",
    "\n",
    "    frame1 = plt.gca()\n",
    "\n",
    "    frame1.axes.get_xaxis().set_visible(False)\n",
    "\n",
    "    frame1.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    plots.append(fig)   \n",
    "\n",
    "    \n",
    "\n",
    "    return plots\n",
    "\n",
    "    #pp.savefig()\n",
    "\n",
    "    \n",
    "\n",
    "    #pp.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_type = 'cumulative'\n",
    "\n",
    "\n",
    "\n",
    "if 'graphplots' not in os.listdir(ROOT_PATH):\n",
    "\n",
    "    os.mkdir(ROOT_PATH+'graphplots')\n",
    "\n",
    "if graph_type not in os.listdir(ROOT_PATH+'graphplots'):\n",
    "\n",
    "    os.mkdir(ROOT_PATH+'graphplots/'+graph_type)\n",
    "\n",
    "    \n",
    "\n",
    "plots_dict = dict()\n",
    "\n",
    "for g_name in G_dict_monthly_cumulative[list(G_dict_monthly_cumulative.keys())[0]].keys():\n",
    "\n",
    "    plots_dict[g_name] = []\n",
    "\n",
    "    \n",
    "\n",
    "for date in G_dict_monthly_cumulative:\n",
    "\n",
    "     for g_name in G_dict_monthly_cumulative[date]:\n",
    "\n",
    "        print(\"======\\n\"+g_name+\"\\n\")\n",
    "\n",
    "        plots_dict[g_name] += compute_distribution_plots(G_dict_monthly_cumulative[date][g_name],date+\"_\"+g_name, graph_type = graph_type)\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_type = 'monthly'\n",
    "\n",
    "    \n",
    "\n",
    "if 'graphplots' not in os.listdir(ROOT_PATH):\n",
    "\n",
    "    os.mkdir(ROOT_PATH+'graphplots')\n",
    "\n",
    "if graph_type not in os.listdir(ROOT_PATH+'graphplots'):\n",
    "\n",
    "    os.mkdir(ROOT_PATH+'graphplots/'+graph_type)\n",
    "\n",
    "    \n",
    "\n",
    "for date in G_dict_monthly:\n",
    "\n",
    "     for g_name in G_dict_monthly[date]:\n",
    "\n",
    "        print(\"======\\n\"+g_name+\"\\n\")\n",
    "\n",
    "        compute_distribution_plots(G_dict_monthly[date][g_name],date+\"_\"+g_name, graph_type = graph_type)\n",
    "\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Scale-free analysis**\n",
    "\n",
    "### **Definition**\n",
    "\n",
    "A network is called scale-free if the characteristics of the network are independent of the size of the network (i.e. the number of nodes). This means that when the network grows, the underlying structure remains the same. A scale-free network is dened by the distribution of the number of edges of the nodes following a so called power law distribution. An example of central concern for macroeconomics are production networks whose scale-free nature has recently been put forward as a potentially major driver of macroeconomic fluctuation\n",
    "\n",
    "### **Small World Theory**\n",
    "\n",
    "Small world theory is based on the idea that two individuals will be connected through a series of intermediaries. In the 1960s, Stanley Milgram tested this theory [TM69] in which all nodes are distance from each other for a short path. The \"Six degrees of separation\" is the idea that all people on average are six, or fewer, social connections away from each other. As a result, a chain of \"friend of a friend\" statements can be made to connect any two people in a maximum of six steps.\n",
    "\n",
    "### **Properties**\n",
    "\n",
    "- Growth: a growth process where, over an extended period of time, new nodes join an already existing system.\n",
    "- Preferential Attachment: In real networks new nodes prefer to link to the more connected nodes.\n",
    "- Scale-free Resiliency: Scale-free networks are more resistant to random disconnection of nodes. It can be eliminated a considerable number of nodes randomly and the network's struc- ture is preserved and will not break into disconnected clusters. When the most connected nodes are targeted, the diameter of a scale-free network increases and the network breaks into isolated clusters. This occurs because when removing these nodes, the damage disturbs the heart of the system, whereas a random attack is most likely not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('SFanalysis/')\n",
    "\n",
    "import sfanalysis as sf\n",
    "\n",
    "\n",
    "\n",
    "import visualisations as vz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gml(df, df_type):\n",
    "\n",
    "    if df_type == 'monthly_cumulative':\n",
    "\n",
    "        if 'gmls' not in os.listdir(ROOT_PATH+\"/SF_results\"):\n",
    "\n",
    "            os.makedirs(ROOT_PATH+\"/SF_results/gmls/\",exist_ok=True)\n",
    "\n",
    "            os.makedirs(ROOT_PATH+\"/SF_results/degseqs/gmls/\",exist_ok=True)\n",
    "\n",
    "            os.mkdir(ROOT_PATH/'gmls')\n",
    "\n",
    "            os.mkdir('degseqs')\n",
    "\n",
    "        folder = ROOT_PATH+'SF_results/gmls'\n",
    "\n",
    "    if df_type == 'monthly':\n",
    "\n",
    "        if 'gmls_month' not in os.listdir(ROOT_PATH+\"/SF_results\"):\n",
    "\n",
    "            os.makedirs(ROOT_PATH+\"/SF_results/gmls_month/\",exist_ok=True)\n",
    "\n",
    "            os.makedirs(ROOT_PATH+\"/SF_results/degseqs_month/gmls_month\",exist_ok=True)\n",
    "\n",
    "        folder = ROOT_PATH+'SF_results/gmls_month'        \n",
    "\n",
    "    if df_type != 'monthly' and df_type != 'monthly_cumulative':\n",
    "\n",
    "        raise Exception(\"Specify the type of dataset: [monthly, monthly_cumulative]\")\n",
    "\n",
    "    for date in df:\n",
    "\n",
    "        for instr in df[date]:\n",
    "\n",
    "            nx.write_gml(df[date][instr], f\"{folder}/{date}_{instr}.gml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gml(G_dict_monthly_cumulative, df_type='monthly_cumulative')\n",
    "\n",
    "create_gml(G_dict_monthly, df_type='monthly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scale_free(df_type, force =False):\n",
    "\n",
    "    \n",
    "\n",
    "    path = ROOT_PATH+'SF_results/'\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "    if df_type =='cumulative':\n",
    "\n",
    "        # location of gml files to analyze\n",
    "\n",
    "        gml_dir = 'gmls/'\n",
    "\n",
    "        # location to write degree sequences\n",
    "\n",
    "        deg_dir = 'degseqs/' \n",
    "\n",
    "        name_csv_file = \"\"\n",
    "\n",
    "    elif df_type =='monthly':\n",
    "\n",
    "        gml_dir = 'gmls_month/'\n",
    "\n",
    "        # location to write degree sequences\n",
    "\n",
    "        deg_dir = 'degseqs_month/'\n",
    "\n",
    "        name_csv_file = \"_monthly\"\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise Exception(\"Specify the type of dataset: [monthly, monthly_cumulative]\")\n",
    "\n",
    "    \n",
    "\n",
    "    # make catalog of gmls and write degree sequence files\n",
    "\n",
    "    # each row of deg_df is a degree sequence file\n",
    "\n",
    "    if not f'export_degree{name_csv_file}.csv' in os.listdir(path) or force:\n",
    "\n",
    "        deg_df = sf.write_degree_sequences(gml_dir, deg_dir)\n",
    "\n",
    "        deg_df.to_csv(f'{path}export_degree{name_csv_file}.csv')\n",
    "\n",
    "    else:\n",
    "\n",
    "        deg_df= pd.read_csv(f'{path}export_degree{name_csv_file}.csv',index_col=[0])\n",
    "\n",
    "        \n",
    "\n",
    "    # analyze all degree sequences (this will take a while for many or large data sets)    \n",
    "\n",
    "    if not f'export_analysis{name_csv_file}.csv' in os.listdir(path) or force:   \n",
    "\n",
    "        analysis_df = sf.analyze_degree_sequences(deg_dir, deg_df)\n",
    "\n",
    "        analysis_df.to_csv(f'{path}export_analysis{name_csv_file}.csv')\n",
    "\n",
    "    else:\n",
    "\n",
    "        analysis_df = pd.read_csv(f'{path}export_analysis{name_csv_file}.csv',index_col=[0]) \n",
    "\n",
    "        \n",
    "\n",
    "    # categorize networks (by unique gml file) into scale-free categories    \n",
    "\n",
    "    if not f'export_hypothesis{name_csv_file}.csv' in os.listdir(path) or force:         \n",
    "\n",
    "        hyps_df = sf.categorize_networks(analysis_df)\n",
    "\n",
    "        hyps_df.to_csv(f'{path}export_hypothesis{name_csv_file}.csv')\n",
    "\n",
    "    else:\n",
    "\n",
    "        hyps_df = pd.read_csv(f'{path}export_hypothesis{name_csv_file}.csv',index_col=[0])\n",
    "\n",
    "\n",
    "\n",
    "    return {'deg':deg_df, 'analysis':analysis_df, 'hyps':hyps_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_sf_dict = get_scale_free(df_type='cumulative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps = cumulative_sf_dict['hyps']\n",
    "\n",
    "not_sf = hyps[(hyps['Strongest'] == False)&(hyps['Strong'] == False)&(hyps['Super_Weak'] == False) & (hyps['Weakest'] == False) & (hyps['Weak'] == False)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.split('/')[1].split('.gml')[0] for i in list(not_sf.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sf_dict = get_scale_free(df_type='monthly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_scalefree_plots(sf_dict):\n",
    "\n",
    "    deg_df = sf_dict['deg']\n",
    "\n",
    "    analysis_df = sf_dict['analysis']\n",
    "\n",
    "    hyps_df = sf_dict['hyps']\n",
    "\n",
    "    \n",
    "\n",
    "    plots = []\n",
    "\n",
    "    not_sf = hyps_df[(hyps_df['Strongest'] == False)&(hyps_df['Strong'] == False)&(hyps_df['Super_Weak'] == False) & (hyps_df['Weakest'] == False) & (hyps_df['Weak'] == False)]\n",
    "\n",
    "    not_sf_all = [i.split('/')[1].split('.gml')[0] for i in list(not_sf.index)]\n",
    "\n",
    "\n",
    "\n",
    "    not_sf_dates = []\n",
    "\n",
    "    not_sf_instr = []\n",
    "\n",
    "    not_sf_sett = []\n",
    "\n",
    "    for i in not_sf_all:\n",
    "\n",
    "        splitting = i.split('_')\n",
    "\n",
    "        not_sf_dates.append(splitting[0])\n",
    "\n",
    "        not_sf_instr.append(splitting[1])\n",
    "\n",
    "        not_sf_sett.append(splitting[2])\n",
    "\n",
    "\n",
    "\n",
    "    not_sf_dates_dict = dict()\n",
    "\n",
    "    for i in not_sf_dates:\n",
    "\n",
    "        not_sf_dates_dict[i] = not_sf_dates.count(i)\n",
    "\n",
    "\n",
    "\n",
    "    not_sf_instr_dict = dict()\n",
    "\n",
    "    for i in not_sf_instr:\n",
    "\n",
    "        not_sf_instr_dict[i] = not_sf_instr.count(i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    not_sf_sett_dict = dict()\n",
    "\n",
    "    for i in not_sf_sett:\n",
    "\n",
    "        not_sf_sett_dict[i] = not_sf_sett.count(i)\n",
    "\n",
    "    \n",
    "\n",
    "    not_sf_dates_df = pd.DataFrame.from_dict(not_sf_dates_dict, orient='index', columns=['num'])\n",
    "\n",
    "    not_sf_instr_df = pd.DataFrame.from_dict(not_sf_instr_dict, orient='index', columns=['num'])\n",
    "\n",
    "    not_sf_sett_df = pd.DataFrame.from_dict(not_sf_sett_dict, orient='index', columns=['num'])\n",
    "\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(15,12))  \n",
    "\n",
    "    p = sns.barplot(x = not_sf_sett_df.index, y =not_sf_sett_df['num'])  \n",
    "\n",
    "    plots.append(p)\n",
    "\n",
    "    fig = plt.figure(figsize=(15,12))  \n",
    "\n",
    "    p = sns.barplot(x = not_sf_instr_df.index, y =not_sf_instr_df['num'])\n",
    "\n",
    "    plots.append(p)\n",
    "\n",
    "    fig = plt.figure(figsize=(15,12)) \n",
    "\n",
    "    ax =sns.barplot(x = not_sf_dates_df.index, y = not_sf_dates_df['num'])\n",
    "\n",
    "#    ax.set(title = \"Emotions Distribution for \"+name, xlabel = \"Emotion\",  ylabel = \"Number\")\n",
    "\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "    plots.append(ax)\n",
    "\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,7))\n",
    "\n",
    "\n",
    "\n",
    "    vz.make_domain_ploth(ax, hyps_df, True)\n",
    "\n",
    "    ax.text(-0.4, 10, 'All data sets' , fontsize=15, rotation=90, va='bottom')\n",
    "\n",
    "    plots.append(ax)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_plots_cum = get_scalefree_plots(cumulative_sf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ROOT_PATH+'SF_results/'\n",
    "\n",
    "with PdfPages(f'{path}/cumulative_scale_free.pdf') as pdf:\n",
    "\n",
    "    for p in sf_plots_cum:      \n",
    "\n",
    "        fig=p.get_figure()\n",
    "\n",
    "        pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_plots_month = get_scalefree_plots(monthly_sf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ROOT_PATH+'SF_results/'\n",
    "\n",
    "with PdfPages(f'{path}/monthly_scale_free.pdf') as pdf:\n",
    "\n",
    "    for p in sf_plots_month:      \n",
    "\n",
    "        fig=p.get_figure()\n",
    "\n",
    "        pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Node Deletion**\n",
    "\n",
    "A Network Resilience Analysis is performed for the monthly aggregated networks. In a network a node deletion means that a Company may goes to bankrupt or most commonly the company decides to exit the system. This deletion may alter the structure of the networks. To verify the structure alterations, it is possible to detect any changes in the Connected Component (CC) of a Network. If after a deletion, a change in the CC is present, this means that the node is a vulnerable one and the Network has been damaged and compromised. If after a deletion, there is no changes in the Connected Component, this implies that the networks has not been compromised. Moreover, the scale-free property strongly correlates with the network's robustness to failure. The major hubs are closely followed by smaller ones. These smaller hubs, in turn, are followed by other nodes with an even smaller degree and so on. Different node deletion approaches are applied:\n",
    "\n",
    "- Random Node deletion: a node is deleted randomly form the Network\n",
    "- Localized Node deletion: a deletion of a precisely selected node If failures occur at random and the vast majority of nodes are those with small degree, the likelihood that a hub would be affected is almost negligible. Localized attacks makes the scale free network more vulnerable compared to random attacks. In this case the targets are the most central nodes with high number of connection and amount counter-value delivered. Network of payments have shown scale-free properties in literature and they are resilient to random damage. This means that it is barely possible to destroy the network of payments by random removal, but if an exact portion of particularly selected nodes is removed, it breaks completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_deletion_breakpoint(G_init):\n",
    "\n",
    "    count_cnt = []\n",
    "\n",
    "    for i in range(1000):\n",
    "\n",
    "        G = G_init.copy()\n",
    "\n",
    "        init_len = len(G)\n",
    "\n",
    "        flag = True\n",
    "\n",
    "        cnt = 0\n",
    "\n",
    "        while flag and len(G)>0:\n",
    "\n",
    "            node_list = list(G.nodes())\n",
    "\n",
    "            rand = random.randint(0,len(node_list)-1)\n",
    "\n",
    "            remove = node_list[rand]\n",
    "\n",
    "            G.remove_node(node_list[rand])\n",
    "\n",
    "            cnt += 1\n",
    "\n",
    "            if len(G)>0:\n",
    "\n",
    "                if not nx.is_weakly_connected(G):\n",
    "\n",
    "                    count_cnt.append(cnt)\n",
    "\n",
    "                    flag = False\n",
    "\n",
    "        \n",
    "\n",
    "    return np.mean(count_cnt)\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoint_dict_cumulative= dict()\n",
    "\n",
    "for month in G_dict_monthly_cumulative:\n",
    "\n",
    "    print(month)\n",
    "\n",
    "    breakpoint_dict_cumulative[month] = dict()\n",
    "\n",
    "    for g_name in G_dict_monthly_cumulative[month]:\n",
    "\n",
    "        breakpoint_dict_cumulative[month][g_name]=random_deletion_breakpoint(G_dict_monthly_cumulative[month][g_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(breakpoint_dict_cumulative, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoint_dict_monthly = dict()\n",
    "\n",
    "for month in G_dict_monthly:\n",
    "\n",
    "    breakpoint_dict_monthly[month] = dict()\n",
    "\n",
    "    for g_name in G_dict_monthly[month]:\n",
    "\n",
    "        breakpoint_dict_monthly[month][g_name]=random_deletion_breakpoint(G_dict_monthly[month][g_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(breakpoint_dict_monthly, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_deletion_distr(deletion_list,feature, name = \"\"):\n",
    "\n",
    "    x = [str(i) for i in sorted(list(deletion_list.keys()))]\n",
    "\n",
    "    y = [deletion_list[i][feature] for i in deletion_list]\n",
    "\n",
    "    sns.set(rc={'figure.figsize':(25,10)}, style=\"white\", font_scale=1.5)\n",
    "\n",
    "    plot  = sns.lineplot(x=x,y=y, linewidth = 3)\n",
    "\n",
    "    plot.set_title(name+\"- Node deleting distribution - \"+feature)\n",
    "\n",
    "    plot.set_ylabel(feature)\n",
    "\n",
    "    plot.set_xlabel(\"Node % deletion\")\n",
    "\n",
    "    plt.legend(labels=['random', 'hub'])\n",
    "\n",
    "\n",
    "\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_deletion_distribution(G_init, save_name=\"\",mode=\"random\",plot_mode='avg_path_length', save=False):\n",
    "\n",
    "    G = G_init.copy()\n",
    "\n",
    "    if mode == \"hub\":\n",
    "\n",
    "        centr = nx.pagerank(G, weight= \"weight\")\n",
    "\n",
    "        sort_orders = sorted(centr.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if mode == \"random\":\n",
    "\n",
    "        sort_orders = list(G.nodes())\n",
    "\n",
    "    perc_dict_random = dict()\n",
    "\n",
    "\n",
    "\n",
    "    init_len = len(G_init)\n",
    "\n",
    "    node_list = list(G.nodes())\n",
    "\n",
    "    five_perc = round(init_len/100*5)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    hub = 0\n",
    "\n",
    "    while hub < len(sort_orders):\n",
    "\n",
    "   # for hub in range(len(sort_orders)):\n",
    "\n",
    "        if cnt%10 == 0:\n",
    "\n",
    "            five_perc-=1\n",
    "\n",
    "        else:\n",
    "\n",
    "            five_perc+=1\n",
    "\n",
    "        if len(G) <= 0 or cnt > 100:\n",
    "\n",
    "                \n",
    "\n",
    "            break\n",
    "\n",
    "            \n",
    "\n",
    "        node_list = list(G.nodes())\n",
    "\n",
    "        \n",
    "\n",
    "        perc_dict_random[cnt] = dict()\n",
    "\n",
    "        perc_dict_random[cnt]['size'] =G.size()\n",
    "\n",
    "        perc_dict_random[cnt]['order'] = G.order()\n",
    "\n",
    "        perc_dict_random[cnt]['GWCC'] = [len(c) for c in sorted(nx.weakly_connected_components(G),key=len, reverse=True)]\n",
    "\n",
    "        perc_dict_random[cnt]['num_GWCC'] = len([len(c) for c in sorted(nx.weakly_connected_components(G),key=len, reverse=True)])\n",
    "\n",
    "        largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
    "\n",
    "        perc_dict_random[cnt]['avg_path_length'] = nx.average_shortest_path_length(G.subgraph(largest_cc).copy())\n",
    "\n",
    "        \n",
    "\n",
    "        counter = 0\n",
    "\n",
    "        while counter < five_perc:\n",
    "\n",
    "     #   for del_num in range(five_perc):\n",
    "\n",
    "            if len(G) > 0:\n",
    "\n",
    "                if mode == \"random\":\n",
    "\n",
    "                    rand = random.randint(0,len(node_list)-1)\n",
    "\n",
    "                    remove = node_list[rand]\n",
    "\n",
    "                    G.remove_node(node_list[rand]) \n",
    "\n",
    "                    node_list = list(G.nodes())\n",
    "\n",
    "                   \n",
    "\n",
    "                if mode == 'hub':\n",
    "\n",
    "                   # print(sort_orders[hub][0])\n",
    "\n",
    "                    G.remove_node(sort_orders[hub][0])\n",
    "\n",
    "    \n",
    "\n",
    "            counter+=1\n",
    "\n",
    "            hub+=1\n",
    "\n",
    " #       print(str(hub)+\" - \"+str(len(sort_orders)))\n",
    "\n",
    "#        hub +=1\n",
    "\n",
    "        cnt += 5\n",
    "\n",
    "        \n",
    "\n",
    "    for key in list(perc_dict_random.keys()):\n",
    "\n",
    "        #print(key)\n",
    "\n",
    "        if key > 100:\n",
    "\n",
    "            perc_dict_random.pop(key)\n",
    "\n",
    "\n",
    "\n",
    "    if save:\n",
    "\n",
    "       # if 'deletion_distr' not in f'{ROOT_PATH}':\n",
    "\n",
    "       #     os.mkdir(ROOT_PATH+'/deletion_distr')\n",
    "\n",
    "        plot_obj = plot_deletion_distr(perc_dict_random, plot_mode)\n",
    "\n",
    "        figure = plot_obj.get_figure() \n",
    "\n",
    "        if mode == 'hub':\n",
    "\n",
    "            figure.savefig(f'{ROOT_PATH}/deletion_distr/{plot_mode}-{save_name}.png')\n",
    "\n",
    "    return perc_dict_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_deletion_monthly = dict()\n",
    "\n",
    "plots = dict()\n",
    "\n",
    "for month in G_dict_monthly:\n",
    "\n",
    "    distr_deletion_monthly[month] = dict()\n",
    "\n",
    "    \n",
    "\n",
    "    print(month)\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    path = ROOT_PATH+'/deletion_distr/monthly/'\n",
    "\n",
    "    for g_name in G_dict_monthly[month]:\n",
    "\n",
    "        \n",
    "\n",
    "        for f in ['avg_path_length', 'size']:\n",
    "\n",
    "            #plt.clf()\n",
    "\n",
    " \n",
    "\n",
    "            for m in ['random','hub']:\n",
    "\n",
    "                distr_deletion_monthly[month][g_name] = node_deletion_distribution(G_dict_monthly[month][g_name], save_name=month+\"_\"+g_name+\"_month\",plot_mode=f,mode=m,save =False)\n",
    "\n",
    "                plot = plot_deletion_distr( distr_deletion_monthly[month][g_name], f, name = month+\"_\"+g_name)\n",
    "\n",
    "            if g_name in plots.keys():\n",
    "\n",
    "                plots[g_name].append(plot)\n",
    "\n",
    "            if g_name not in plots.keys():\n",
    "\n",
    "                plots[g_name] = [plot]\n",
    "\n",
    "               \n",
    "\n",
    "\n",
    "\n",
    "            #plots.append(plot)\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "for key in plots:\n",
    "\n",
    "\n",
    "\n",
    "    with PdfPages(f'{path}{key}_avgpath_size.pdf') as pdf:\n",
    "\n",
    "        for p in plots[key]:      \n",
    "\n",
    "            fig=p.get_figure()\n",
    "\n",
    "            pdf.savefig(fig)         \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_deletion_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_deletion_cumulative = dict()\n",
    "\n",
    "plots = dict()\n",
    "\n",
    "for month in G_dict_monthly_cumulative:\n",
    "\n",
    "    distr_deletion_cumulative[month] = dict()\n",
    "\n",
    "    \n",
    "\n",
    "    print(month)\n",
    "\n",
    "    path = ROOT_PATH+'/deletion_distr/cumulative/'\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    for g_name in G_dict_monthly_cumulative[month]:\n",
    "\n",
    "        \n",
    "\n",
    "        for f in ['avg_path_length', 'size']:\n",
    "\n",
    "            #plt.clf()\n",
    "\n",
    " \n",
    "\n",
    "            for m in ['random','hub']:\n",
    "\n",
    "                distr_deletion_cumulative[month][g_name] = node_deletion_distribution(G_dict_monthly_cumulative[month][g_name], save_name=month+\"_\"+g_name+\"_cumulative\",plot_mode=f,mode=m,save =False)\n",
    "\n",
    "                plot = plot_deletion_distr( distr_deletion_cumulative[month][g_name], f, name = month+\"_\"+g_name)\n",
    "\n",
    "            if g_name in plots.keys():\n",
    "\n",
    "                plots[g_name].append(plot)\n",
    "\n",
    "            if g_name not in plots.keys():\n",
    "\n",
    "                plots[g_name] = [plot]\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "for key in plots:\n",
    "\n",
    "\n",
    "\n",
    "    with PdfPages(f'{path}{key}_avgpath_size.pdf') as pdf:\n",
    "\n",
    "        for p in plots[key]:      \n",
    "\n",
    "            fig=p.get_figure()\n",
    "\n",
    "            pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_deletion_cumulative = dict()\n",
    "\n",
    "for month in G_dict_monthly_cumulative:\n",
    "\n",
    "    distr_deletion_cumulative[month] = dict()\n",
    "\n",
    "    print(month)\n",
    "\n",
    "    for g_name in G_dict_monthly_cumulative[month]:\n",
    "\n",
    "        for f in ['avg_path_length', 'size']:\n",
    "\n",
    "            plt.clf()\n",
    "\n",
    "            for m in ['random','hub']:\n",
    "\n",
    "                distr_deletion_cumulative[month][g_name] = node_deletion_distribution(G_dict_monthly_cumulative[month][g_name], save_name=month+\"_\"+g_name+\"_cum\",plot_mode=f,mode=m,save =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_deletion_cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hub_deletion_importance(G_init,name):\n",
    "\n",
    "    #(G_init, ,mode=\"random\",plot_mode='avg_path_length', save=False):\n",
    "\n",
    "    G = G_init.copy()\n",
    "\n",
    "    init_len = len(G)\n",
    "\n",
    "\n",
    "\n",
    "    perc_dict_hub_sing = dict()\n",
    "\n",
    "\n",
    "\n",
    "    #perc_dict_hub_three = dict()\n",
    "\n",
    "    centr = nx.pagerank(G, weight= \"weight\")\n",
    "\n",
    "    sort_orders = sorted(centr.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "    for hub in sort_orders[:10]:\n",
    "\n",
    "        G = G_init.copy()\n",
    "\n",
    "        G.remove_node(hub[0])\n",
    "\n",
    "        #print(\"removing;\",hub[0])\n",
    "\n",
    "        node_list = list(G.nodes())\n",
    "\n",
    "        perc_dict_hub_sing[hub] = dict()\n",
    "\n",
    "        perc_dict_hub_sing[hub]['size'] =G.size()\n",
    "\n",
    "        perc_dict_hub_sing[hub]['order'] = G.order()\n",
    "\n",
    "        perc_dict_hub_sing[hub]['GWCC'] = [len(c) for c in sorted(nx.weakly_connected_components(G),key=len, reverse=True)]\n",
    "\n",
    "        perc_dict_hub_sing[hub]['num_GWCC'] = len([len(c) for c in sorted(nx.weakly_connected_components(G),key=len, reverse=True)])\n",
    "\n",
    "        if len(G) != 0:\n",
    "\n",
    "            if G.is_directed() and nx.is_weakly_connected(G):\n",
    "\n",
    "                perc_dict_hub_sing[hub]['avg_path_length'] = nx.average_shortest_path_length(G)\n",
    "\n",
    "                \n",
    "\n",
    "    return perc_dict_hub_sing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_del_importance_month = dict()\n",
    "\n",
    "for date in G_dict_monthly:\n",
    "\n",
    "    hub_del_importance_month[date] = dict()\n",
    "\n",
    "    for g_name in G_dict_monthly[date]:   \n",
    "\n",
    "        hub_del_importance_month[date][g_name] = hub_deletion_importance(\n",
    "\n",
    "                G_dict_monthly[date][g_name], name = date+\"_\"+g_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_del_importance_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_del_importance_cumulative = dict()\n",
    "\n",
    "for date in G_dict_monthly_cumulative:\n",
    "\n",
    "    hub_del_importance_cumulative[date] = dict()\n",
    "\n",
    "    for g_name in G_dict_monthly_cumulative[date]:   \n",
    "\n",
    "        hub_del_importance_cumulative[date][g_name] = hub_deletion_importance(\n",
    "\n",
    "                G_dict_monthly_cumulative[date][g_name], name = date+\"_\"+g_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_del_companies_importance(df):\n",
    "\n",
    "    count_dict = dict()\n",
    "\n",
    "    for d in df:\n",
    "\n",
    "        for g in df[d]:\n",
    "\n",
    "            for comp in df[d][g]:\n",
    "\n",
    "               # print(hub_del_importance_cumulative[d][g][comp])\n",
    "\n",
    "               # break\n",
    "\n",
    "                if df[d][g][comp]['num_GWCC'] >1:\n",
    "\n",
    "                    #print(d)\n",
    "\n",
    "                    #print(g)\n",
    "\n",
    "                    #print(comp[0])\n",
    "\n",
    "                    #print(hub_del_importance_cumulative[d][g][comp]['GWCC'])\n",
    "\n",
    "                    #print(hub_del_importance_cumulative[d][g][comp])\n",
    "\n",
    "                    #print('\\n\\n')\n",
    "\n",
    "                    #if comp[0] == 'INTESA SANPAOLO':\n",
    "\n",
    "                    #    print(comp[0])\n",
    "\n",
    "                    if comp[0] not in count_dict:\n",
    "\n",
    "                        count_dict[comp[0]] = 1\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        count_dict[comp[0]] += 1\n",
    "\n",
    "    count_dict = {k: v for k, v in sorted(count_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    fig = plt.figure(figsize=(15,12)) \n",
    "\n",
    "    ax =sns.barplot(x =  list(count_dict.keys()), y = list(count_dict.values()))\n",
    "\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_del_companies_importance(hub_del_importance_cumulative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_del_companies_importance(hub_del_importance_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Plot functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_graph_plot_nx(G, layout='spring_layout'):\n",
    "\n",
    "    if layout =='kamada_kawai_layout':\n",
    "\n",
    "        pos = nx.kamada_kawai_layout(G)\n",
    "\n",
    "    if layout =='circular_layout':      \n",
    "\n",
    "        pos = nx.circular_layout(G)\n",
    "\n",
    "    if layout == 'spring_layout':\n",
    "\n",
    "        pos = nx.spring_layout(G, k=100000,iterations=5000)\n",
    "\n",
    "    \n",
    "\n",
    "    if layout == 'random_layout':\n",
    "\n",
    "        pos = nx.random_layout(G)\n",
    "\n",
    "    \n",
    "\n",
    "    degree = list(dict(G.degree()).values())\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "\n",
    "    color_list = [ [128,167,240,0.6] for i in list(G.nodes())]\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_size= [i*20+100 for i in degree], node_color= [ '#%02x%02x%02xB3' % (c[0],c[1],c[2]) for c in color_list ] )\n",
    "\n",
    "    cmap = plt.cm.plasma\n",
    "\n",
    "    ax= plt.gca()\n",
    "\n",
    "    ax.collections[0].set_edgecolor(\"black\")\n",
    "\n",
    "    nx.draw_networkx_edges(G, pos,width=2, alpha=0.25, arrowstyle=\"->\",\n",
    "\n",
    "    arrowsize=10,\n",
    "\n",
    "    edge_color='#93e685'            \n",
    "\n",
    "   # edge_cmap=cmap,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    for y in pos:\n",
    "\n",
    "        pos[y][1] = pos[y][1]-0.1\n",
    "\n",
    "    nx.draw_networkx_labels(G,pos)\n",
    "\n",
    "  #  pos = nx.spring_layout(G)\n",
    "\n",
    "    \n",
    "\n",
    "  #  pos_attrs = {}\n",
    "\n",
    "  #  for node, coords in pos.items():\n",
    "\n",
    "  #      pos[node] = (coords[0], coords[1] + 0.08)\n",
    "\n",
    "  #      nx.draw_networkx_labels(G,pos,font_size=8)\n",
    "\n",
    "   # labels = nx.get_edge_attributes(G,'weight')\n",
    "\n",
    "   # nx.draw_networkx_edge_labels(G,pos,edge_labels=labels,font_color='r',font_size=10)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def compute_graph_plot_ig(G, show_hubs=True, hubs_type=\"degree\",save=False, name = None):\n",
    "\n",
    "    g = ig.Graph.from_networkx(G)\n",
    "\n",
    "    g.vs[\"label\"] = list(G.nodes())\n",
    "\n",
    "    if len(G.nodes())==0 or len(G.edges())== 0:\n",
    "\n",
    "        return\n",
    "\n",
    "    degree = list(G.degree())\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "    g.vs[\"label_size\"] = 10\n",
    "\n",
    "    g.vs[\"label_dist\"] =1.2\n",
    "\n",
    "    g.vs[\"label_color\"] = \"rgba(219, 10, 91, 0.8)\"\n",
    "\n",
    "    g.vs['size'] = [i/5+10 for i in list(dict(G.degree()).values())]\n",
    "\n",
    "    g.es['width'] = 0.5\n",
    "\n",
    "    g.es['arrow_size'] = 0.3\n",
    "\n",
    "    g.es['color'] = 'gray'\n",
    "\n",
    "    g.vs['color'] = \"rgba(128,167,240,0.6)\" \n",
    "\n",
    "\n",
    "\n",
    "    if show_hubs:\n",
    "\n",
    "        if hubs_type == 'degree':\n",
    "\n",
    "            degree = list(dict(G.degree()).values())\n",
    "\n",
    "            dic = dict(G.degree()).items()\n",
    "\n",
    "        if hubs_type == 'in':\n",
    "\n",
    "            degree = list(dict(G.in_degree()).values())\n",
    "\n",
    "            dic = dict(G.in_degree()).items()\n",
    "\n",
    "        if hubs_type == 'out':\n",
    "\n",
    "            degree = list(dict(G.out_degree()).values())\n",
    "\n",
    "            dic = dict(G.out_degree()).items()\n",
    "\n",
    "            \n",
    "\n",
    "        if hubs_type == 'degree_w':\n",
    "\n",
    "            degree = list(dict(G.degree(weight=\"weight\")).values())\n",
    "\n",
    "            dic = dict(G.degree(weight=\"weight\")).items()\n",
    "\n",
    "        if hubs_type == 'in_w':\n",
    "\n",
    "            degree = list(dict(G.in_degree(weight=\"weight\")).values())\n",
    "\n",
    "            dic = dict(G.in_degree(weight=\"weight\")).items()\n",
    "\n",
    "        if hubs_type == 'out_w':\n",
    "\n",
    "            degree = list(dict(G.out_degree(weight=\"weight\")).values())\n",
    "\n",
    "            dic = dict(G.out_degree(weight=\"weight\")).items()\n",
    "\n",
    "            \n",
    "\n",
    "        percentile_90 = np.percentile(degree,90)   \n",
    "\n",
    "        hub_nodi = [k for k,v in dic if v>= percentile_90]\n",
    "\n",
    "        hub = [g.vs['label'].index(i) for i in hub_nodi]\n",
    "\n",
    "    \n",
    "\n",
    "        if len(hub) >0 :\n",
    "\n",
    "            for i in hub:\n",
    "\n",
    "                g.vs[i]['color'] = \"rgba(31, 58, 147, 0.8)\"\n",
    "\n",
    "                g.vs[i]['label'] = list(G.nodes())[i]\n",
    "\n",
    "                g.vs[i]['size'] = degree[i] #degree.index\n",
    "\n",
    "                #g.es[i]['width'] = 1\n",
    "\n",
    "                #g.es[i]['color'] = 'white'\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    coords = ig.Graph.layout_graphopt(g,spring_length=50,node_mass=50,node_charge=10)\n",
    "\n",
    "    if save:\n",
    "\n",
    "      #  if 'igplots' not in os.listdir(ROOT_PATH):\n",
    "\n",
    "      #      os.mkdir(ROOT_PATH+'igplots')\n",
    "\n",
    "                \n",
    "\n",
    "        out = ig.plot(g,layout = coords,  bbox=(1024,1024),margin = 150)\n",
    "\n",
    "        #out = ig.plot(g,layout = coords)\n",
    "\n",
    "        out.save(f'{ROOT_PATH}igplots/{name}.png')\n",
    "\n",
    "       # .save(f'{ROOT_PATH}igplots/{name}.png')\n",
    "\n",
    "   # ig.plot(tig,layout = coords, vertex_color=tig.degree(),vertex_size=[i/5+10 for i in tig.degree()])\n",
    "\n",
    "    return g#ig.plot(g,layout = coords,  margin = 50)\n",
    "\n",
    "\n",
    "\n",
    "def plot_part(g):\n",
    "\n",
    "    g.graph.es['width'] = 0.3\n",
    "\n",
    "    g.graph.es['color'] = 'gray'\n",
    "\n",
    "\n",
    "\n",
    "    g.graph.es['arrow_size'] = 0.3\n",
    "\n",
    "    #g.graph.vs['color'] = None\n",
    "\n",
    "    g.graph.vs[\"label_dist\"] =1.2\n",
    "\n",
    "    g.graph.vs[\"label_color\"] = \"rgba(219, 10, 91, 0.8)\"\n",
    "\n",
    "    g.graph.vs[\"label_size\"] =6\n",
    "\n",
    "    return ig.plot(g)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_graph_plot_partition(G,g,partition, partition_name, save=False,mark_group=False):\n",
    "\n",
    "    \n",
    "\n",
    "    #g = partition\n",
    "\n",
    "    #colors = []\n",
    "\n",
    "   # g = ig.Graph.from_networkx(G)\n",
    "\n",
    "    if isinstance(partition,ig.clustering.VertexDendrogram):\n",
    "\n",
    "                partition = partition.as_clustering()\n",
    "\n",
    "    partition.graph.vs[\"label\"] = list(G.nodes())\n",
    "\n",
    "    partition.graph.vs[\"label_size\"] = 10\n",
    "\n",
    "    partition.graph.vs[\"label_dist\"] =2\n",
    "\n",
    "    partition.graph.vs[\"label_color\"] = \"rgba(219, 10, 91, 0.8)\"\n",
    "\n",
    "    partition.graph.vs['size'] = [i/5+10 for i in list(dict(G.degree()).values())]\n",
    "\n",
    "    partition.graph.es['width'] = 0.5\n",
    "\n",
    "    partition.graph.es['arrow_size'] = 0.3\n",
    "\n",
    "    partition.graph.es['color'] = 'gray'\n",
    "\n",
    "     \n",
    "\n",
    "    pal = ig.drawing.colors.ClusterColoringPalette(len(partition))\n",
    "\n",
    "    partition.graph.vs['color'] = pal.get_many(partition.membership)\n",
    "\n",
    "    #cmap =  cm.get_cmap('turbo')\n",
    "\n",
    "    #for col in colors:\n",
    "\n",
    "    #    col[4] = 0.6\n",
    "\n",
    "    #\n",
    "\n",
    "    #cmap = cm.get_cmap('turbo',max(partition.membership)+1)   # PiYG\n",
    "\n",
    "    #import matplotlib\n",
    "\n",
    "    #colors = []\n",
    "\n",
    "    #for i in range(cmap.N):\n",
    "\n",
    "        #rgba = cmap(i)\n",
    "\n",
    "        #rgba = list(rgba)\n",
    "\n",
    "        #rgba[3] = 0.6\n",
    "\n",
    "       \n",
    "\n",
    "        #colors.append(\"rgba\"+str(tuple(rgba)))\n",
    "\n",
    "        # rgb2hex accepts rgb or rgba\n",
    "\n",
    "        #print(matplotlib.colors.rgb2hex(rgba))\n",
    "\n",
    "    #colors = [colors[i] for i in partition.membership]\n",
    "\n",
    "\n",
    "\n",
    "    #partition.graph.vs['color'] = colors #[i.append(0.6) for i in colors]\n",
    "\n",
    "    #print(colors)\n",
    "\n",
    "    #coords = ig.Graph.layout_fruchterman_reingold(g.graph, weights=[i/100000000000 for i in g.graph.es['weight']])\n",
    "\n",
    "    coords = ig.Graph.layout_graphopt(partition.graph,spring_length=50,node_mass=50,node_charge=10)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    out = ig.plot(partition.graph,layout = coords, bbox=(1024,1024),margin = 150,mark_groups = mark_group)\n",
    "\n",
    "    if save:\n",
    "\n",
    "        if 'communitiesplots' not in os.listdir(ROOT_PATH):\n",
    "\n",
    "            os.mkdir(ROOT_PATH+'communitiesplots')\n",
    "\n",
    "        #out = ig.plot(partition,layout = coords,margin = 50)\n",
    "\n",
    "        out.save(f'{ROOT_PATH}communitiesplots/{g_name}_comm_{partition_name}.png')\n",
    "\n",
    "        \n",
    "\n",
    "    return out #ig.plot(partition,layout = coords,  margin = 50)\n",
    "\n",
    "\n",
    "\n",
    "def compute_graph__plot_pyvis(G):\n",
    "\n",
    "    degree = list(dict(G.degree()).values())\n",
    "\n",
    "\n",
    "\n",
    "    percentile_99 = np.percentile(degree,99)\n",
    "\n",
    "    hub_nodi = [k for k,v in dict(G.degree()).items() if v>= percentile_99]\n",
    "\n",
    "    \n",
    "\n",
    "    nt = Network(height='100%', width='100%', bgcolor='#222222', font_color='white')#, notebook =True)\n",
    "\n",
    "    nt.force_atlas_2based(gravity=-300,spring_length=300)\n",
    "\n",
    "    nt.from_nx(G)\n",
    "\n",
    "    for i in range(len(nt.nodes)):\n",
    "\n",
    "        nt.nodes[i]['size'] = dict(G.degree)[nt.nodes[i]['id']]\n",
    "\n",
    "        if nt.nodes[i]['id'] in hub_nodi:\n",
    "\n",
    "            nt.nodes[i]['color'] = '#93e685'\n",
    "\n",
    "    \n",
    "\n",
    "    nt.show('nx.html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = []\n",
    "\n",
    "os.makedirs(ROOT_PATH+'igplots/monthly', exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "for date in G_dict_monthly.keys():\n",
    "\n",
    "    for g_name in G_dict_monthly[date].keys():\n",
    "\n",
    "        p = compute_graph_plot_ig(G_dict_monthly[date][g_name], save = True, name = f'monthly/monthly_plot_{date}{g_name}')\n",
    "\n",
    "        \n",
    "\n",
    "      #  if p is not None:\n",
    "\n",
    "       #     p.save(f'{ROOT_PATH}igplots/monthly/monthly_plot_{date}{g_name}.png') \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = []\n",
    "\n",
    "os.makedirs(ROOT_PATH+'igplots/monthly', exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "for date in G_dict_monthly.keys():\n",
    "\n",
    "    for g_name in G_dict_monthly[date].keys():\n",
    "\n",
    "        p = compute_graph_plot_ig(G_dict_monthly[date][g_name], save = True, name = f'monthly/monthly_plot_{date}{g_name}')\n",
    "\n",
    "        \n",
    "\n",
    "      #  if p is not None:\n",
    "\n",
    "       #     p.save(f'{ROOT_PATH}igplots/monthly/monthly_plot_{date}{g_name}.png') \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
